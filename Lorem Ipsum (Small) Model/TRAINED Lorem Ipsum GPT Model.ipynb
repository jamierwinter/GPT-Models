{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01e9c164",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:17:19.234711Z",
     "iopub.status.busy": "2025-03-31T01:17:19.234711Z",
     "iopub.status.idle": "2025-03-31T01:17:24.098008Z",
     "shell.execute_reply": "2025-03-31T01:17:24.095186Z",
     "shell.execute_reply.started": "2025-03-31T01:17:19.234711Z"
    }
   },
   "outputs": [],
   "source": [
    "#Model to generate Lorem Ipsum\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "612399b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:17:24.105708Z",
     "iopub.status.busy": "2025-03-31T01:17:24.104362Z",
     "iopub.status.idle": "2025-03-31T01:17:24.237708Z",
     "shell.execute_reply": "2025-03-31T01:17:24.236607Z",
     "shell.execute_reply.started": "2025-03-31T01:17:24.105708Z"
    }
   },
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "BatchSize = 32\n",
    "BlockSize = 128\n",
    "Dmodel = 192\n",
    "nheads = 4\n",
    "Dk = int(Dmodel/nheads)\n",
    "Dv = Dk\n",
    "LearningRate = 1e-3\n",
    "MaxIters = 2500\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ContextLength=500\n",
    "TransformerBlocks = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d39483cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:17:24.239598Z",
     "iopub.status.busy": "2025-03-31T01:17:24.238990Z",
     "iopub.status.idle": "2025-03-31T01:17:24.254051Z",
     "shell.execute_reply": "2025-03-31T01:17:24.248417Z",
     "shell.execute_reply.started": "2025-03-31T01:17:24.239544Z"
    }
   },
   "outputs": [],
   "source": [
    "#Decode string from tokens\n",
    "def decode(tokens):\n",
    "    str = []\n",
    "    index = 0\n",
    "    offset = 0\n",
    "    while index - offset < len(tokens):\n",
    "        offset = 0\n",
    "        token = tokens[index]\n",
    "        #print(token)\n",
    "        if token in Initialvocab:\n",
    "            str.append(token)\n",
    "        else:\n",
    "            #print(merges)\n",
    "            if token in merges.values():\n",
    "                 for key, value in merges.items():\n",
    "                    if value == token:\n",
    "                        #print(token)\n",
    "                        tokens.insert(index + 1, key[0])\n",
    "                        tokens.insert(index + 2, key[1])\n",
    "                        offset = 1\n",
    "                        break\n",
    "        index +=1\n",
    "    return bytes(str).decode(\"utf-8\", errors=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58c427f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:17:24.254051Z",
     "iopub.status.busy": "2025-03-31T01:17:24.254051Z",
     "iopub.status.idle": "2025-03-31T01:17:24.282484Z",
     "shell.execute_reply": "2025-03-31T01:17:24.280230Z",
     "shell.execute_reply.started": "2025-03-31T01:17:24.254051Z"
    }
   },
   "outputs": [],
   "source": [
    "def getPairFreqs(text):\n",
    "    freqs = {}\n",
    "    for pair in zip(text, text[1:]):\n",
    "        try:\n",
    "            freqs[pair] +=1\n",
    "        except KeyError:\n",
    "            freqs[pair] = 1\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f3c5cbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:17:24.288809Z",
     "iopub.status.busy": "2025-03-31T01:17:24.287821Z",
     "iopub.status.idle": "2025-03-31T01:17:24.297130Z",
     "shell.execute_reply": "2025-03-31T01:17:24.296085Z",
     "shell.execute_reply.started": "2025-03-31T01:17:24.288809Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge(text, pair, newChar):\n",
    "    newText = []\n",
    "    i=0\n",
    "    while i < len(text):\n",
    "        if i < len(text) - 1 and (text[i], text[i+1]) == (pair[0], pair[1]):\n",
    "            newText.append(newChar)\n",
    "            i+=2\n",
    "        else:\n",
    "            newText.append(text[i])\n",
    "            i+=1\n",
    "    return newText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c7d4039",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:17:24.299530Z",
     "iopub.status.busy": "2025-03-31T01:17:24.298733Z",
     "iopub.status.idle": "2025-03-31T01:17:24.640286Z",
     "shell.execute_reply": "2025-03-31T01:17:24.638698Z",
     "shell.execute_reply.started": "2025-03-31T01:17:24.299491Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load variables from pickle files (Lorem Ipsum)\n",
    "with open('LoremIpsuminitialvocab.pkl', 'rb') as f:\n",
    "    Initialvocab = pickle.load(f)\n",
    "    \n",
    "with open('LoremIpsumvocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "    \n",
    "with open('LoremIpsumvocabsize.pkl', 'rb') as f:\n",
    "    VocabSize = pickle.load(f)\n",
    "    \n",
    "with open('LoremIpsummerges.pkl', 'rb') as f:\n",
    "    merges = pickle.load(f)\n",
    "    \n",
    "with open('LoremIpsumtrainingdata.pkl', 'rb') as f:\n",
    "    TrainingData = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97574bb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:17:24.642832Z",
     "iopub.status.busy": "2025-03-31T01:17:24.641033Z",
     "iopub.status.idle": "2025-03-31T01:17:24.656427Z",
     "shell.execute_reply": "2025-03-31T01:17:24.655021Z",
     "shell.execute_reply.started": "2025-03-31T01:17:24.642797Z"
    }
   },
   "outputs": [],
   "source": [
    "#Encode string to tokens\n",
    "def encode(text):\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "    while True:\n",
    "        freqs = getPairFreqs(tokens)\n",
    "        pairIndex = float(\"inf\")\n",
    "        pairToMerge=\"\"\n",
    "        for pair in freqs.keys():\n",
    "            if merges.get(pair, float(\"inf\")) < pairIndex:\n",
    "                pairIndex = merges.get(pair, float(\"inf\"))\n",
    "                pairToMerge = pair\n",
    "        if pairIndex == float(\"inf\"):\n",
    "            break\n",
    "        tokens = merge(tokens, pairToMerge, pairIndex)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "296a5c38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:17:24.658868Z",
     "iopub.status.busy": "2025-03-31T01:17:24.658130Z",
     "iopub.status.idle": "2025-03-31T01:17:24.683287Z",
     "shell.execute_reply": "2025-03-31T01:17:24.681983Z",
     "shell.execute_reply.started": "2025-03-31T01:17:24.658823Z"
    }
   },
   "outputs": [],
   "source": [
    "#Currently we don't use any ReGex but I will see if I want to change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b182cf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:17:24.685764Z",
     "iopub.status.busy": "2025-03-31T01:17:24.684968Z",
     "iopub.status.idle": "2025-03-31T01:17:24.701221Z",
     "shell.execute_reply": "2025-03-31T01:17:24.700108Z",
     "shell.execute_reply.started": "2025-03-31T01:17:24.685719Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create Training/Testing Data Batches\n",
    "def GetBatch(split):\n",
    "    offsets = []\n",
    "    for i in range(0, BatchSize):\n",
    "        offsets.append(random.randint(0, len(TrainingData) - BlockSize - 1))\n",
    "    for i in range(0, BatchSize):\n",
    "        x = torch.stack([torch.tensor(TrainingData[i:i+BlockSize]) for i in offsets])\n",
    "        y = torch.stack([torch.tensor(TrainingData[i+1:i+BlockSize+1]) for i in offsets])\n",
    "    return x, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d66389a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:17:24.703694Z",
     "iopub.status.busy": "2025-03-31T01:17:24.703041Z",
     "iopub.status.idle": "2025-03-31T01:17:24.735581Z",
     "shell.execute_reply": "2025-03-31T01:17:24.734339Z",
     "shell.execute_reply.started": "2025-03-31T01:17:24.703657Z"
    }
   },
   "outputs": [],
   "source": [
    "#Single Attention Head\n",
    "class AttentionHead(nn.Module):\n",
    "    \n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        #Learned key matrix\n",
    "        self.Wk = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((Dmodel, Dk), device=device)), requires_grad=True)\n",
    "        #Learned query matrix\n",
    "        self.Wq = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((Dmodel, Dk), device=device)), requires_grad=True)\n",
    "        #Learned value matrix\n",
    "        self.Wv = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((Dmodel, Dv), device=device)), requires_grad=True)\n",
    "    \n",
    "    def forward(self, E):\n",
    "        #print(E.shape)\n",
    "        #Dynamically checks size of E as it is not always \n",
    "        Batches, Blocks = E.shape[0], E.shape[1]\n",
    "        #Compute key, query, value matrices\n",
    "        Q = E @ self.Wq\n",
    "        K = E @ self.Wk\n",
    "        V = E @ self.Wv\n",
    "        #Masking matrix\n",
    "        M = torch.tril(torch.zeros(Blocks, Blocks)).masked_fill(torch.tril(torch.ones(Blocks, Blocks)) == 0, float(\"-inf\")).to(device)\n",
    "        #Compute attention pattern\n",
    "        AttentionPattern = torch.nn.functional.softmax(((Q @ K.transpose(-2,-1))/((Dk)**(1/2)) + M), dim = -1) @ V\n",
    "        #Residual connection\n",
    "        return AttentionPattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9414b931",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:17:24.738120Z",
     "iopub.status.busy": "2025-03-31T01:17:24.737333Z",
     "iopub.status.idle": "2025-03-31T01:17:24.782586Z",
     "shell.execute_reply": "2025-03-31T01:17:24.781265Z",
     "shell.execute_reply.started": "2025-03-31T01:17:24.738077Z"
    }
   },
   "outputs": [],
   "source": [
    "#Multi-headed Attention\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead() for i in range(nheads)])\n",
    "        \n",
    "    def forward(self, E):\n",
    "        #Concatenate multiple heads of attention\n",
    "        return torch.cat([head(E) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ee5be2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:17:24.785054Z",
     "iopub.status.busy": "2025-03-31T01:17:24.784338Z",
     "iopub.status.idle": "2025-03-31T01:17:24.894794Z",
     "shell.execute_reply": "2025-03-31T01:17:24.893521Z",
     "shell.execute_reply.started": "2025-03-31T01:17:24.785010Z"
    }
   },
   "outputs": [],
   "source": [
    "#Feedforward layer\n",
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ffl = nn.Sequential(\n",
    "            #Each MLP has 4* more neurons than there are dimensions .\n",
    "            nn.Linear(Dmodel, Dmodel * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(Dmodel * 4, Dmodel),\n",
    "        )\n",
    "\n",
    "    def forward(self, E):\n",
    "        return self.ffl(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55a42855",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:17:24.897310Z",
     "iopub.status.busy": "2025-03-31T01:17:24.896460Z",
     "iopub.status.idle": "2025-03-31T01:17:24.915683Z",
     "shell.execute_reply": "2025-03-31T01:17:24.914457Z",
     "shell.execute_reply.started": "2025-03-31T01:17:24.897310Z"
    }
   },
   "outputs": [],
   "source": [
    "#Transformer Block\n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.AttentionHeads = MultiHeadedAttention()\n",
    "        self.ffl = FeedForwardLayer()\n",
    "        self.LayerNorm1 = nn.LayerNorm(Dmodel, device=device)\n",
    "        self.LayerNorm2 = nn.LayerNorm(Dmodel, device=device)\n",
    "    \n",
    "    #Slightly different to my formalisation as do layernorm before blocks, not after\n",
    "    #The original did layernorm second, but it is more common practice to do layernorm first now\n",
    "    def forward(self, E):\n",
    "        E = E + self.AttentionHeads(self.LayerNorm1(E))\n",
    "        E = E + self.ffl(self.LayerNorm2(E))\n",
    "        return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d177a8ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:17:24.925010Z",
     "iopub.status.busy": "2025-03-31T01:17:24.919737Z",
     "iopub.status.idle": "2025-03-31T01:17:24.942097Z",
     "shell.execute_reply": "2025-03-31T01:17:24.940734Z",
     "shell.execute_reply.started": "2025-03-31T01:17:24.925003Z"
    }
   },
   "outputs": [],
   "source": [
    "#Unembedding Layer. We don't focus on the last embedding at this stage unlike in my formalisation. We can do that when we need to\n",
    "class UnembedLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        #Learned unembedding matrix\n",
    "        self.Wu = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((Dmodel, VocabSize), device=device)), requires_grad=True)\n",
    "        \n",
    "    def forward(self, E):\n",
    "        logits = E @ self.Wu\n",
    "        #Dynamically check shape of logits, since this isnt always BatchSize, BlockSize (e.g. when generating text)\n",
    "        Batches, Blocks = logits.shape[0], logits.shape[1]\n",
    "        #Converts logits to a single list of logits for compatibility with cross entropy functional\n",
    "        logits = logits.view(Batches*Blocks, VocabSize)\n",
    "        #probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "369f773b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:17:24.944859Z",
     "iopub.status.busy": "2025-03-31T01:17:24.944006Z",
     "iopub.status.idle": "2025-03-31T01:17:25.020673Z",
     "shell.execute_reply": "2025-03-31T01:17:25.019510Z",
     "shell.execute_reply.started": "2025-03-31T01:17:24.944835Z"
    }
   },
   "outputs": [],
   "source": [
    "#Transformer Implementation\n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        #Learned Embedding matrix. Requires_grad ensures We updated during backpropagation.\n",
    "        self.We = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((VocabSize, Dmodel), device=device)), requires_grad=True)\n",
    "        #Learned positional encoding matrix. Dimension n x d_model, where n = BlockSize\n",
    "        self.Wp = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((ContextLength, Dmodel), device=device)), requires_grad=True)\n",
    "        #Unembeding layer\n",
    "        self.UnembedLayer = UnembedLayer()\n",
    "        #Transformer blocks\n",
    "        self.Blocks = nn.Sequential(*[Block() for i in range(TransformerBlocks)])\n",
    "        \n",
    "        #Having O = None means O is optional. We don't always want targets since when generating text we don't have targets.\n",
    "    def forward(self, I, O = None):\n",
    "        #I has shape BatchSize x BlockSize\n",
    "        #One hot vector for tokens. Shape BatchSize x BlockSize x VocabSize\n",
    "        U = torch.nn.functional.one_hot(torch.tensor(I).to(device), VocabSize).float()\n",
    "        #Initial token embeddings. Shape BatchSize x BlockSize x d_model\n",
    "        E = U @ self.We\n",
    "        #Adds another dimension to Wp so that it is now 1 x BlockSize x d_model and can be added to E\n",
    "        P = self.Wp.unsqueeze(0)  \n",
    "\n",
    "        #Adds positional encoding to embedding\n",
    "        E = E + P[:, :E.shape[1], :] #truncates the positional encoding matrix to only be as long as the number of embeddings in E\n",
    "        E = self.Blocks(E)\n",
    "        #print(E[1][1])\n",
    "        logits = self.UnembedLayer(E).to(device)\n",
    "        if O is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            #Converts O to a single list of expected outputs for compatibility with cross entropy functional\n",
    "            O = O.view(BlockSize * BatchSize).to(device)\n",
    "            #Cross entropy loss calculated on the raw logits rather than Softmaxed logits.\n",
    "            #In theory, Cross entropy should be calculated on the Softmaxed logits, but\n",
    "            #the cross_entropy function in python is defined to take in raw logits\n",
    "            #If you try and pass the softmaxed logits in (as I originally tried), you will get\n",
    "            #Vanishing gradient and your network wont train\n",
    "            loss = torch.nn.functional.cross_entropy(logits, O)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generateText(self, I, Length):\n",
    "        for i in range(Length):\n",
    "            #Get predictions\n",
    "            logits, loss = self(I)\n",
    "            #Get probs\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            #Focus on prediction for next token\n",
    "            probs = probs[-1, :]\n",
    "            #Sample from next token distribution\n",
    "            nextToken = torch.multinomial(probs, num_samples = 1).unsqueeze(0)\n",
    "            #print(I)\n",
    "            #Concatenate next token to current text\n",
    "            I = torch.cat((I.to(device), nextToken.to(device)), dim=1)\n",
    "        return I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43dbecda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:17:25.023534Z",
     "iopub.status.busy": "2025-03-31T01:17:25.022672Z",
     "iopub.status.idle": "2025-03-31T01:17:26.210764Z",
     "shell.execute_reply": "2025-03-31T01:17:26.209468Z",
     "shell.execute_reply.started": "2025-03-31T01:17:25.023493Z"
    }
   },
   "outputs": [],
   "source": [
    "T = Transformer()\n",
    "T = T.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02731aa8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:17:26.213734Z",
     "iopub.status.busy": "2025-03-31T01:17:26.210764Z",
     "iopub.status.idle": "2025-03-31T01:17:28.678784Z",
     "shell.execute_reply": "2025-03-31T01:17:28.675996Z",
     "shell.execute_reply.started": "2025-03-31T01:17:26.213734Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 2713728\n"
     ]
    }
   ],
   "source": [
    "#Model training\n",
    "#Prints parameters\n",
    "#print(list(T.parameters()))\n",
    "#Prints number of trainable parameters\n",
    "print(\"Number of trainable parameters: \" + str((sum(p.numel() for p in T.parameters() if p.requires_grad))))\n",
    "#Create optimiser object\n",
    "optimiser = torch.optim.AdamW(T.parameters(), lr=LearningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6945b085-030a-4aa1-a881-5d982075274a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:17:28.681259Z",
     "iopub.status.busy": "2025-03-31T01:17:28.680648Z",
     "iopub.status.idle": "2025-03-31T01:18:52.089474Z",
     "shell.execute_reply": "2025-03-31T01:18:52.069871Z",
     "shell.execute_reply.started": "2025-03-31T01:17:28.681259Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1015/3863220517.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  U = torch.nn.functional.one_hot(torch.tensor(I).to(device), VocabSize).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.0766, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "ultricies posuere, erspiciatis sed . Dolad. Vestibulum eu rhonPellentesque optioucimus ivamus turprehender. Proin quis aperinim nesciunt. tempor medolores . Praesent metus. Nullam Q. Aliquam erat volutpat. et aut us. ReIn ac ste nisi. Donec nemo nisl. Fusce quis libero vestibulum elit. rutrtem Eos esercommodo suscipit potent, beataeconguVoluptalanmaximpretium, dapibConsequatur consecteturLabvoluptateus. Donec eros. Fusce perferendismagniLorem ipsum dolor sit amet, consectetuer adipiscing elit. Aenean commodo ligula eget dolor. Aenean massa. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Donec quam felis, ultricies nec, pellentesque eu, pretium quis, sem. Nulla consequat massa quis enim. Donec pede justo, fringilla vel, aliquet nec, vulputate eget, arcu. In enim justo, rhoncus ut, imperdiet a, venenatis vitae, justo. Nullam dictum felis eu pede mollis pretium. Integer tincidunt. Cras dapibus. Vivamus elementum semper nisi. Aenean vulputate eleifend tellus. Aenean leo ligula, porttitor eu, consequat vitae, eleifend ac, enim. Aliquam lorem ante, dapibus in, viverra convallis, ore opmpedit . hac habitasse platednatoque penatibus et magnis dis parturient t magna Reem fuganisi, ea erat ut consequatur est.\n",
      "\n",
      "corruptincidunmieuistincideiTempmaisapiente,animfelis in nibhorci blandiarcu quis eataaepatat eleifend , id erat. leo, nunc et dapibus natoLorem ipsum dolor sit amet, consectetuer adipiscing elit. Aenean commodo ligula eget dolor. Aenean massa. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Donec quam felis, ultricies nec, pellentesque eu, pretium quis, sem. Nulla consequat massa quis enim. Donec pede justo, eniscleo. Maecenas borpellentesque . Hic nisl . Ipsam dolor. Aenean elit sit amet facermod ucimus assumendadolore natoque penatibus et magnis dis parturient montes, nascetur ridiculnostrumgiLorem ipsum dolor sit amet, consectetuer adipiscing elit. Aenean commodo ligula eget dolor. Aenean massa. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Donec quam felis, ultricies nec, pellentesque eu, pretium quis, sem. Nulla consequat massa quis enim. Donec pede justo, fringilla vel, aliquet nec, vulputate eget, arcu. In enim justo, rhoncus ut, imperdiet a, venenatis vitae, justo. Nullam dictum felis eu pede mollis pretium. Integer tincidunt. Cras dapibus. Vivamus elementum semper nisi. Aenean vulputate eleifend tellus. Aenean leo ligula, porttitor eu, consequat vitae, eleifend ac, enim. Aliquam lorem ante, dapibus in, viverra quis, feugiat a, tellus. Phasellus viverra nulla ut metus varius laoreet. Quisque rutrum. Aenean imperdiet. Etiam ultricies nisi vel augue. Curabitur ullamcorper ultricies nisi. Nam eget dui. Etiam rhoncus. Maecenas tempus, tellus eget condimentum rhoncus, sem quam semper libero, sit amet adipiscing sem neque sed ipsum. Nam et dolor enarecusandae . Quam natoque penatibus et magnis dis parturient montes, nascetur ridiculus ultricrepellat iores et ut Nteger ante ipsum mo us. Donec lorem. Aperiam ventore In estilibero. Praesent Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia dui. Aenean ut cupiditate re\n"
     ]
    }
   ],
   "source": [
    "#Generate Text: Pre-Training\n",
    "\n",
    "#Check loss\n",
    "I, O = GetBatch(\"training\")\n",
    "logits, loss = T(I, O)\n",
    "print(loss)\n",
    "#Generates text of given length\n",
    "tokens = sum(T.generateText(torch.zeros(1,1).long(), 150).tolist(), [])\n",
    "print(decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b13b052c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:18:52.098316Z",
     "iopub.status.busy": "2025-03-31T01:18:52.097459Z",
     "iopub.status.idle": "2025-03-31T01:46:47.361457Z",
     "shell.execute_reply": "2025-03-31T01:46:47.357848Z",
     "shell.execute_reply.started": "2025-03-31T01:18:52.098316Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1015/3863220517.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  U = torch.nn.functional.one_hot(torch.tensor(I).to(device), VocabSize).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: tensor(8.0679, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "0.8769590854644775 seconds elapsed\n",
      "100: tensor(5.7087, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "77.75233554840088 seconds elapsed\n",
      "200: tensor(5.2385, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "80.63928413391113 seconds elapsed\n",
      "300: tensor(4.6289, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "71.76940274238586 seconds elapsed\n",
      "400: tensor(4.8228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "65.31927943229675 seconds elapsed\n",
      "500: tensor(4.4791, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "63.77346181869507 seconds elapsed\n",
      "600: tensor(4.5268, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "72.19815707206726 seconds elapsed\n",
      "700: tensor(3.7593, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "72.72682332992554 seconds elapsed\n",
      "800: tensor(3.4650, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "62.02445030212402 seconds elapsed\n",
      "900: tensor(3.1614, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "61.53569197654724 seconds elapsed\n",
      "1000: tensor(2.7836, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "71.30098128318787 seconds elapsed\n",
      "1100: tensor(2.1003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "70.95184874534607 seconds elapsed\n",
      "1200: tensor(1.7331, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "63.00259971618652 seconds elapsed\n",
      "1300: tensor(1.4674, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "54.77270555496216 seconds elapsed\n",
      "1400: tensor(1.1776, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "62.510587215423584 seconds elapsed\n",
      "1500: tensor(1.1347, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "59.307435750961304 seconds elapsed\n",
      "1600: tensor(0.8620, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "64.10258340835571 seconds elapsed\n",
      "1700: tensor(0.7584, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "68.77342677116394 seconds elapsed\n",
      "1800: tensor(0.6135, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "69.65052103996277 seconds elapsed\n",
      "1900: tensor(0.6023, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "69.89374899864197 seconds elapsed\n",
      "2000: tensor(0.5649, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "70.48294830322266 seconds elapsed\n",
      "2100: tensor(0.4643, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "66.64091610908508 seconds elapsed\n",
      "2200: tensor(0.4516, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "63.738446950912476 seconds elapsed\n",
      "2300: tensor(0.4309, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "59.754417419433594 seconds elapsed\n",
      "2400: tensor(0.3843, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "61.45426869392395 seconds elapsed\n",
      "2500: tensor(0.3791, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "70.19279599189758 seconds elapsed\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "start_time = time.time()\n",
    "for iters in range(MaxIters + 1):\n",
    "    #GetBatches\n",
    "    I, O = GetBatch(\"training\") #This works\n",
    "    #Get loss from Transformer pass\n",
    "    logits, loss = T(I, O) \n",
    "    optimiser.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    if iters % 100 == 0:\n",
    "        new_time = time.time()\n",
    "        print(str(iters) + \": \" + str(loss))\n",
    "        print(str(new_time - start_time) + \" seconds elapsed\")\n",
    "        start_time = start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "538a1d53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:46:47.364733Z",
     "iopub.status.busy": "2025-03-31T01:46:47.363219Z",
     "iopub.status.idle": "2025-03-31T01:47:53.925213Z",
     "shell.execute_reply": "2025-03-31T01:47:53.923413Z",
     "shell.execute_reply.started": "2025-03-31T01:46:47.364622Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1015/3863220517.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  U = torch.nn.functional.one_hot(torch.tensor(I).to(device), VocabSize).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4161, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "mattis ut. Et venenatis sed nulla sed tempor. Sed commodo posuere eleifend. Donec quis dolor commodo facilisis nisi. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia curae; Ut varius, arcu a tempor metus, quis placerat felis quam vitae felis. Aenean finibus, est eros sit amet semper sem, a erat. Quisque quam eu posuere enim. Aenean cursus nisi sit amet lacus et est facilisis lacinia. Duis leo nibh, vel tincidunt in, commodo enim. Integer pulvinar, dolor a venenatis nisi. Pellentesque consequat ante vitae erat lacus. Ut semper quam in feugiat. Duis convallis rutrum. Quisque suscipit lacinia nisl sit amet commodo odio, a fermentum enim. Pellentesque consectetur vulputate lectus, non scelerisque mi dolor vel consectetur eu. Suspendisse ut mauris faucibus, et pretium magna gravida. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Etiam nunc risus, ornare in semper ac, auctor sed ante. Nulla vitae mi congue, accumsan sapien sodales. Sed nibh erat, dignissim quis mauris et, tincidunt Quisque consequat rhoncus. Quisque consequat fringilla libero orci, in purus sit amet ex .\n",
      "\n",
      "Maecenas quis mauris nibh, quis quam dui, nec \n"
     ]
    }
   ],
   "source": [
    "#Generate Text: Post-Training\n",
    "\n",
    "#Check loss\n",
    "I, O = GetBatch(\"training\")\n",
    "logits, loss = T(I, O)\n",
    "print(loss)\n",
    "#Generates text of given length\n",
    "tokens = sum(T.generateText(torch.zeros(1,1).long(), 150).tolist(), [])\n",
    "print(decode(tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
