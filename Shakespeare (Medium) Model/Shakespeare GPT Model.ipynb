{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01e9c164",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:15:23.469455Z",
     "iopub.status.busy": "2025-03-31T01:15:23.469455Z",
     "iopub.status.idle": "2025-03-31T01:15:28.561506Z",
     "shell.execute_reply": "2025-03-31T01:15:28.560054Z",
     "shell.execute_reply.started": "2025-03-31T01:15:23.469455Z"
    }
   },
   "outputs": [],
   "source": [
    "#Model to generate Lorem Ipsum\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "612399b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:15:28.565026Z",
     "iopub.status.busy": "2025-03-31T01:15:28.563755Z",
     "iopub.status.idle": "2025-03-31T01:15:28.635435Z",
     "shell.execute_reply": "2025-03-31T01:15:28.634043Z",
     "shell.execute_reply.started": "2025-03-31T01:15:28.564975Z"
    }
   },
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "BatchSize = 64\n",
    "BlockSize = 256\n",
    "Dmodel = 384\n",
    "nheads = 6\n",
    "Dk = int(Dmodel/nheads)\n",
    "Dv = Dk\n",
    "LearningRate = 3e-4\n",
    "MaxIters = 5000\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ContextLength=500\n",
    "TransformerBlocks = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d39483cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:15:28.665300Z",
     "iopub.status.busy": "2025-03-31T01:15:28.636915Z",
     "iopub.status.idle": "2025-03-31T01:15:28.674896Z",
     "shell.execute_reply": "2025-03-31T01:15:28.673124Z",
     "shell.execute_reply.started": "2025-03-31T01:15:28.665174Z"
    }
   },
   "outputs": [],
   "source": [
    "#Decode string from tokens\n",
    "def decode(tokens):\n",
    "    str = []\n",
    "    index = 0\n",
    "    offset = 0\n",
    "    while index - offset < len(tokens):\n",
    "        offset = 0\n",
    "        token = tokens[index]\n",
    "        #print(token)\n",
    "        if token in Initialvocab:\n",
    "            str.append(token)\n",
    "        else:\n",
    "            #print(merges)\n",
    "            if token in merges.values():\n",
    "                 for key, value in merges.items():\n",
    "                    if value == token:\n",
    "                        #print(token)\n",
    "                        tokens.insert(index + 1, key[0])\n",
    "                        tokens.insert(index + 2, key[1])\n",
    "                        offset = 1\n",
    "                        break\n",
    "        index +=1\n",
    "    return bytes(str).decode(\"utf-8\", errors=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58c427f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:15:28.677738Z",
     "iopub.status.busy": "2025-03-31T01:15:28.676667Z",
     "iopub.status.idle": "2025-03-31T01:15:28.766548Z",
     "shell.execute_reply": "2025-03-31T01:15:28.765453Z",
     "shell.execute_reply.started": "2025-03-31T01:15:28.677666Z"
    }
   },
   "outputs": [],
   "source": [
    "def getPairFreqs(text):\n",
    "    freqs = {}\n",
    "    for pair in zip(text, text[1:]):\n",
    "        try:\n",
    "            freqs[pair] +=1\n",
    "        except KeyError:\n",
    "            freqs[pair] = 1\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f3c5cbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:15:28.771424Z",
     "iopub.status.busy": "2025-03-31T01:15:28.769686Z",
     "iopub.status.idle": "2025-03-31T01:15:28.783297Z",
     "shell.execute_reply": "2025-03-31T01:15:28.781876Z",
     "shell.execute_reply.started": "2025-03-31T01:15:28.771424Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge(text, pair, newChar):\n",
    "    newText = []\n",
    "    i=0\n",
    "    while i < len(text):\n",
    "        if i < len(text) - 1 and (text[i], text[i+1]) == (pair[0], pair[1]):\n",
    "            newText.append(newChar)\n",
    "            i+=2\n",
    "        else:\n",
    "            newText.append(text[i])\n",
    "            i+=1\n",
    "    return newText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c7d4039",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:15:28.784819Z",
     "iopub.status.busy": "2025-03-31T01:15:28.784819Z",
     "iopub.status.idle": "2025-03-31T01:15:28.879032Z",
     "shell.execute_reply": "2025-03-31T01:15:28.877383Z",
     "shell.execute_reply.started": "2025-03-31T01:15:28.784819Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load variables from pickle files (Lorem Ipsum)\n",
    "with open('Shakespeareinitialvocab.pkl', 'rb') as f:\n",
    "    Initialvocab = pickle.load(f)\n",
    "    \n",
    "with open('Shakespearevocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "    \n",
    "with open('Shakespearevocabsize.pkl', 'rb') as f:\n",
    "    VocabSize = pickle.load(f)\n",
    "    \n",
    "with open('Shakespearemerges.pkl', 'rb') as f:\n",
    "    merges = pickle.load(f)\n",
    "    \n",
    "with open('Shakespearetrainingdata.pkl', 'rb') as f:\n",
    "    TrainingData = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97574bb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:15:28.881950Z",
     "iopub.status.busy": "2025-03-31T01:15:28.880715Z",
     "iopub.status.idle": "2025-03-31T01:15:28.890318Z",
     "shell.execute_reply": "2025-03-31T01:15:28.888877Z",
     "shell.execute_reply.started": "2025-03-31T01:15:28.881877Z"
    }
   },
   "outputs": [],
   "source": [
    "#Encode string to tokens\n",
    "def encode(text):\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "    while True:\n",
    "        freqs = getPairFreqs(tokens)\n",
    "        pairIndex = float(\"inf\")\n",
    "        pairToMerge=\"\"\n",
    "        for pair in freqs.keys():\n",
    "            if merges.get(pair, float(\"inf\")) < pairIndex:\n",
    "                pairIndex = merges.get(pair, float(\"inf\"))\n",
    "                pairToMerge = pair\n",
    "        if pairIndex == float(\"inf\"):\n",
    "            break\n",
    "        tokens = merge(tokens, pairToMerge, pairIndex)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "296a5c38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:15:28.893327Z",
     "iopub.status.busy": "2025-03-31T01:15:28.891716Z",
     "iopub.status.idle": "2025-03-31T01:15:28.902534Z",
     "shell.execute_reply": "2025-03-31T01:15:28.901096Z",
     "shell.execute_reply.started": "2025-03-31T01:15:28.893063Z"
    }
   },
   "outputs": [],
   "source": [
    "#Currently we don't use any ReGex but I will see if I want to change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b182cf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:15:28.905254Z",
     "iopub.status.busy": "2025-03-31T01:15:28.904003Z",
     "iopub.status.idle": "2025-03-31T01:15:28.914718Z",
     "shell.execute_reply": "2025-03-31T01:15:28.912943Z",
     "shell.execute_reply.started": "2025-03-31T01:15:28.905199Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create Training/Testing Data Batches\n",
    "def GetBatch(split):\n",
    "    offsets = []\n",
    "    for i in range(0, BatchSize):\n",
    "        offsets.append(random.randint(0, len(TrainingData) - BlockSize - 1))\n",
    "    for i in range(0, BatchSize):\n",
    "        x = torch.stack([torch.tensor(TrainingData[i:i+BlockSize]) for i in offsets])\n",
    "        y = torch.stack([torch.tensor(TrainingData[i+1:i+BlockSize+1]) for i in offsets])\n",
    "    return x, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d66389a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:15:28.922175Z",
     "iopub.status.busy": "2025-03-31T01:15:28.920126Z",
     "iopub.status.idle": "2025-03-31T01:15:28.935568Z",
     "shell.execute_reply": "2025-03-31T01:15:28.934109Z",
     "shell.execute_reply.started": "2025-03-31T01:15:28.922146Z"
    }
   },
   "outputs": [],
   "source": [
    "#Single Attention Head\n",
    "class AttentionHead(nn.Module):\n",
    "    \n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        #Learned key matrix\n",
    "        self.Wk = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((Dmodel, Dk), device=device)), requires_grad=True)\n",
    "        #Learned query matrix\n",
    "        self.Wq = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((Dmodel, Dk), device=device)), requires_grad=True)\n",
    "        #Learned value matrix\n",
    "        self.Wv = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((Dmodel, Dv), device=device)), requires_grad=True)\n",
    "    \n",
    "    def forward(self, E):\n",
    "        #print(E.shape)\n",
    "        #Dynamically checks size of E as it is not always \n",
    "        Batches, Blocks = E.shape[0], E.shape[1]\n",
    "        #Compute key, query, value matrices\n",
    "        Q = E @ self.Wq\n",
    "        K = E @ self.Wk\n",
    "        V = E @ self.Wv\n",
    "        #Masking matrix\n",
    "        M = torch.tril(torch.zeros(Blocks, Blocks)).masked_fill(torch.tril(torch.ones(Blocks, Blocks)) == 0, float(\"-inf\")).to(device)\n",
    "        #Compute attention pattern\n",
    "        AttentionPattern = torch.nn.functional.softmax(((Q @ K.transpose(-2,-1))/((Dk)**(1/2)) + M), dim = -1) @ V\n",
    "        #Residual connection\n",
    "        return AttentionPattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9414b931",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:15:28.938349Z",
     "iopub.status.busy": "2025-03-31T01:15:28.936902Z",
     "iopub.status.idle": "2025-03-31T01:15:28.952931Z",
     "shell.execute_reply": "2025-03-31T01:15:28.951793Z",
     "shell.execute_reply.started": "2025-03-31T01:15:28.938300Z"
    }
   },
   "outputs": [],
   "source": [
    "#Multi-headed Attention\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead() for i in range(nheads)])\n",
    "        \n",
    "    def forward(self, E):\n",
    "        #Concatenate multiple heads of attention\n",
    "        return torch.cat([head(E) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ee5be2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:15:28.957905Z",
     "iopub.status.busy": "2025-03-31T01:15:28.954584Z",
     "iopub.status.idle": "2025-03-31T01:15:28.977809Z",
     "shell.execute_reply": "2025-03-31T01:15:28.977809Z",
     "shell.execute_reply.started": "2025-03-31T01:15:28.957779Z"
    }
   },
   "outputs": [],
   "source": [
    "#Feedforward layer\n",
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ffl = nn.Sequential(\n",
    "            #Each MLP has 4* more neurons than there are dimensions .\n",
    "            nn.Linear(Dmodel, Dmodel * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(Dmodel * 4, Dmodel),\n",
    "        )\n",
    "\n",
    "    def forward(self, E):\n",
    "        return self.ffl(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55a42855",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:15:28.979424Z",
     "iopub.status.busy": "2025-03-31T01:15:28.977809Z",
     "iopub.status.idle": "2025-03-31T01:15:29.028906Z",
     "shell.execute_reply": "2025-03-31T01:15:29.027186Z",
     "shell.execute_reply.started": "2025-03-31T01:15:28.979361Z"
    }
   },
   "outputs": [],
   "source": [
    "#Transformer Block\n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.AttentionHeads = MultiHeadedAttention()\n",
    "        self.ffl = FeedForwardLayer()\n",
    "        self.LayerNorm1 = nn.LayerNorm(Dmodel, device=device)\n",
    "        self.LayerNorm2 = nn.LayerNorm(Dmodel, device=device)\n",
    "    \n",
    "    #Slightly different to my formalisation as do layernorm before blocks, not after\n",
    "    #The original paper did indeed do layernorm second, so switch it back to this\n",
    "    def forward(self, E):\n",
    "        E = E + self.AttentionHeads(self.LayerNorm1(E))\n",
    "        E = E + self.ffl(self.LayerNorm2(E))\n",
    "        return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d177a8ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:15:29.033587Z",
     "iopub.status.busy": "2025-03-31T01:15:29.032824Z",
     "iopub.status.idle": "2025-03-31T01:15:29.047149Z",
     "shell.execute_reply": "2025-03-31T01:15:29.045880Z",
     "shell.execute_reply.started": "2025-03-31T01:15:29.033577Z"
    }
   },
   "outputs": [],
   "source": [
    "#Unembedding Layer. We don't focus on the last embedding at this stage unlike in my formalisation. We can do that when we need to\n",
    "class UnembedLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        #Learned unembedding matrix\n",
    "        self.Wu = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((Dmodel, VocabSize), device=device)), requires_grad=True)\n",
    "        \n",
    "    def forward(self, E):\n",
    "        logits = E @ self.Wu\n",
    "        #Dynamically check shape of logits, since this isnt always BatchSize, BlockSize (e.g. when generating text)\n",
    "        Batches, Blocks = logits.shape[0], logits.shape[1]\n",
    "        #Converts logits to a single list of logits for compatibility with cross entropy functional\n",
    "        logits = logits.view(Batches*Blocks, VocabSize)\n",
    "        #probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "369f773b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:15:29.049083Z",
     "iopub.status.busy": "2025-03-31T01:15:29.048466Z",
     "iopub.status.idle": "2025-03-31T01:15:29.260742Z",
     "shell.execute_reply": "2025-03-31T01:15:29.259722Z",
     "shell.execute_reply.started": "2025-03-31T01:15:29.049083Z"
    }
   },
   "outputs": [],
   "source": [
    "#Transformer Implementation\n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        #Learned Embedding matrix. Requires_grad ensures We updated during backpropagation.\n",
    "        self.We = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((VocabSize, Dmodel), device=device)), requires_grad=True)\n",
    "        #Learned positional encoding matrix. Dimension n x d_model, where n = BlockSize\n",
    "        self.Wp = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((ContextLength, Dmodel), device=device)), requires_grad=True)\n",
    "        #Unembeding layer\n",
    "        self.UnembedLayer = UnembedLayer()\n",
    "        #Transformer blocks\n",
    "        self.Blocks = nn.Sequential(*[Block() for i in range(TransformerBlocks)])\n",
    "        \n",
    "        #Having O = None means O is optional. We don't always want targets since when generating text we don't have targets.\n",
    "    def forward(self, I, O = None):\n",
    "        #I has shape BatchSize x BlockSize\n",
    "        #One hot vector for tokens. Shape BatchSize x BlockSize x VocabSize\n",
    "        U = torch.nn.functional.one_hot(torch.tensor(I).to(device), VocabSize).float()\n",
    "        #Initial token embeddings. Shape BatchSize x BlockSize x d_model\n",
    "        E = U @ self.We\n",
    "        #Adds another dimension to Wp so that it is now 1 x BlockSize x d_model and can be added to E\n",
    "        P = self.Wp.unsqueeze(0)  \n",
    "\n",
    "        #Adds positional encoding to embedding\n",
    "        E = E + P[:, :E.shape[1], :] #truncates the positional encoding matrix to only be as long as the number of embeddings in E\n",
    "        E = self.Blocks(E)\n",
    "        #print(E[1][1])\n",
    "        logits = self.UnembedLayer(E).to(device)\n",
    "        if O is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            #Converts O to a single list of expected outputs for compatibility with cross entropy functional\n",
    "            O = O.view(BlockSize * BatchSize).to(device)\n",
    "            #Cross entropy loss calculated on the raw logits rather than Softmaxed logits.\n",
    "            #In theory, Cross entropy should be calculated on the Softmaxed logits, but\n",
    "            #the cross_entropy function in python is defined to take in raw logits\n",
    "            #If you try and pass the softmaxed logits in (as I originally tried), you will get\n",
    "            #Vanishing gradient and your network wont train\n",
    "            loss = torch.nn.functional.cross_entropy(logits, O)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generateText(self, I, Length):\n",
    "        for i in range(Length):\n",
    "            #Get predictions\n",
    "            logits, loss = self(I)\n",
    "            #Get probs\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            #Focus on prediction for next token\n",
    "            probs = probs[-1, :]\n",
    "            #Sample from next token distribution\n",
    "            nextToken = torch.multinomial(probs, num_samples = 1).unsqueeze(0)\n",
    "            #print(I)\n",
    "            #Concatenate next token to current text\n",
    "            I = torch.cat((I.to(device), nextToken.to(device)), dim=1)\n",
    "        return I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43dbecda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:15:29.272177Z",
     "iopub.status.busy": "2025-03-31T01:15:29.270273Z",
     "iopub.status.idle": "2025-03-31T01:15:30.215013Z",
     "shell.execute_reply": "2025-03-31T01:15:30.209456Z",
     "shell.execute_reply.started": "2025-03-31T01:15:29.272177Z"
    }
   },
   "outputs": [],
   "source": [
    "T = Transformer()\n",
    "T = T.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02731aa8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:15:30.222509Z",
     "iopub.status.busy": "2025-03-31T01:15:30.222509Z",
     "iopub.status.idle": "2025-03-31T01:15:32.066921Z",
     "shell.execute_reply": "2025-03-31T01:15:32.065307Z",
     "shell.execute_reply.started": "2025-03-31T01:15:30.222509Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 13829376\n"
     ]
    }
   ],
   "source": [
    "#Model training\n",
    "#Prints parameters\n",
    "#print(list(T.parameters()))\n",
    "#Prints number of trainable parameters\n",
    "print(\"Number of trainable parameters: \" + str((sum(p.numel() for p in T.parameters() if p.requires_grad))))\n",
    "#Create optimiser object\n",
    "optimiser = torch.optim.AdamW(T.parameters(), lr=LearningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6945b085-030a-4aa1-a881-5d982075274a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:15:32.069913Z",
     "iopub.status.busy": "2025-03-31T01:15:32.068540Z",
     "iopub.status.idle": "2025-03-31T01:15:32.775198Z",
     "shell.execute_reply": "2025-03-31T01:15:32.773932Z",
     "shell.execute_reply.started": "2025-03-31T01:15:32.069800Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Generate Text: Pre-Training\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#Check loss\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m I, O \u001b[38;5;241m=\u001b[39m \u001b[43mGetBatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m T(I, O)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m, in \u001b[0;36mGetBatch\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m      3\u001b[0m offsets \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, BatchSize):\n\u001b[0;32m----> 5\u001b[0m     offsets\u001b[38;5;241m.\u001b[39mappend(random\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mdata\u001b[49m) \u001b[38;5;241m-\u001b[39m BlockSize \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, BatchSize):\n\u001b[1;32m      7\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([torch\u001b[38;5;241m.\u001b[39mtensor(TrainingData[i:i\u001b[38;5;241m+\u001b[39mBlockSize]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m offsets])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "#Generate Text: Pre-Training\n",
    "\n",
    "#Check loss\n",
    "I, O = GetBatch(\"training\")\n",
    "logits, loss = T(I, O)\n",
    "print(loss)\n",
    "#Generates text of given length\n",
    "tokens = sum(T.generateText(torch.zeros(1,1).long(), 150).tolist(), [])\n",
    "print(decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13b052c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-31T01:15:32.776174Z",
     "iopub.status.idle": "2025-03-31T01:15:32.780435Z",
     "shell.execute_reply": "2025-03-31T01:15:32.778483Z",
     "shell.execute_reply.started": "2025-03-31T01:15:32.778444Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Train the model\n",
    "start_time = time.time()\n",
    "for iters in range(MaxIters + 1):\n",
    "    #GetBatches\n",
    "    I, O = GetBatch(\"training\") #This works\n",
    "    #Get loss from Transformer pass\n",
    "    logits, loss = T(I, O) \n",
    "    optimiser.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    if iters % 100 == 0:\n",
    "        new_time = time.time()\n",
    "        print(str(iters) + \": \" + str(loss))\n",
    "        print(str(new_time - start_time) + \" seconds elapsed\")\n",
    "        start_time = start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538a1d53",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-31T01:15:32.783442Z",
     "iopub.status.idle": "2025-03-31T01:15:32.810220Z",
     "shell.execute_reply": "2025-03-31T01:15:32.810220Z",
     "shell.execute_reply.started": "2025-03-31T01:15:32.810220Z"
    }
   },
   "outputs": [],
   "source": [
    "#Generate Text: Post-Training\n",
    "\n",
    "#Check loss\n",
    "I, O = GetBatch(\"training\")\n",
    "logits, loss = T(I, O)\n",
    "print(loss)\n",
    "#Generates text of given length\n",
    "tokens = sum(T.generateText(torch.zeros(1,1).long(), 150).tolist(), [])\n",
    "print(decode(tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
