{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01e9c164",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:58:07.793713Z",
     "iopub.status.busy": "2025-03-31T01:58:07.793578Z",
     "iopub.status.idle": "2025-03-31T01:58:12.625064Z",
     "shell.execute_reply": "2025-03-31T01:58:12.623291Z",
     "shell.execute_reply.started": "2025-03-31T01:58:07.793713Z"
    }
   },
   "outputs": [],
   "source": [
    "#Model to generate Lorem Ipsum\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "612399b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:58:12.629157Z",
     "iopub.status.busy": "2025-03-31T01:58:12.628412Z",
     "iopub.status.idle": "2025-03-31T01:58:12.653909Z",
     "shell.execute_reply": "2025-03-31T01:58:12.647017Z",
     "shell.execute_reply.started": "2025-03-31T01:58:12.629098Z"
    }
   },
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "BatchSize = 64\n",
    "BlockSize = 256\n",
    "Dmodel = 384\n",
    "nheads = 6\n",
    "Dk = int(Dmodel/nheads)\n",
    "Dv = Dk\n",
    "LearningRate = 3e-4\n",
    "MaxIters = 3000\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ContextLength=500\n",
    "TransformerBlocks = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d39483cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:58:12.653909Z",
     "iopub.status.busy": "2025-03-31T01:58:12.653909Z",
     "iopub.status.idle": "2025-03-31T01:58:12.672060Z",
     "shell.execute_reply": "2025-03-31T01:58:12.670279Z",
     "shell.execute_reply.started": "2025-03-31T01:58:12.653909Z"
    }
   },
   "outputs": [],
   "source": [
    "#Decode string from tokens\n",
    "def decode(tokens):\n",
    "    str = []\n",
    "    index = 0\n",
    "    offset = 0\n",
    "    while index - offset < len(tokens):\n",
    "        offset = 0\n",
    "        token = tokens[index]\n",
    "        #print(token)\n",
    "        if token in Initialvocab:\n",
    "            str.append(token)\n",
    "        else:\n",
    "            #print(merges)\n",
    "            if token in merges.values():\n",
    "                 for key, value in merges.items():\n",
    "                    if value == token:\n",
    "                        #print(token)\n",
    "                        tokens.insert(index + 1, key[0])\n",
    "                        tokens.insert(index + 2, key[1])\n",
    "                        offset = 1\n",
    "                        break\n",
    "        index +=1\n",
    "    return bytes(str).decode(\"utf-8\", errors=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58c427f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:58:12.674395Z",
     "iopub.status.busy": "2025-03-31T01:58:12.673597Z",
     "iopub.status.idle": "2025-03-31T01:58:12.726661Z",
     "shell.execute_reply": "2025-03-31T01:58:12.724468Z",
     "shell.execute_reply.started": "2025-03-31T01:58:12.674350Z"
    }
   },
   "outputs": [],
   "source": [
    "def getPairFreqs(text):\n",
    "    freqs = {}\n",
    "    for pair in zip(text, text[1:]):\n",
    "        try:\n",
    "            freqs[pair] +=1\n",
    "        except KeyError:\n",
    "            freqs[pair] = 1\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f3c5cbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:58:12.733030Z",
     "iopub.status.busy": "2025-03-31T01:58:12.731433Z",
     "iopub.status.idle": "2025-03-31T01:58:12.744952Z",
     "shell.execute_reply": "2025-03-31T01:58:12.740887Z",
     "shell.execute_reply.started": "2025-03-31T01:58:12.732956Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge(text, pair, newChar):\n",
    "    newText = []\n",
    "    i=0\n",
    "    while i < len(text):\n",
    "        if i < len(text) - 1 and (text[i], text[i+1]) == (pair[0], pair[1]):\n",
    "            newText.append(newChar)\n",
    "            i+=2\n",
    "        else:\n",
    "            newText.append(text[i])\n",
    "            i+=1\n",
    "    return newText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c7d4039",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:58:12.750747Z",
     "iopub.status.busy": "2025-03-31T01:58:12.748555Z",
     "iopub.status.idle": "2025-03-31T01:58:12.881469Z",
     "shell.execute_reply": "2025-03-31T01:58:12.881469Z",
     "shell.execute_reply.started": "2025-03-31T01:58:12.750664Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load variables from pickle files (Lorem Ipsum)\n",
    "with open('Shakespeareinitialvocab.pkl', 'rb') as f:\n",
    "    Initialvocab = pickle.load(f)\n",
    "    \n",
    "with open('Shakespearevocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "    \n",
    "with open('Shakespearevocabsize.pkl', 'rb') as f:\n",
    "    VocabSize = pickle.load(f)\n",
    "    \n",
    "with open('Shakespearemerges.pkl', 'rb') as f:\n",
    "    merges = pickle.load(f)\n",
    "    \n",
    "with open('Shakespearetrainingdata.pkl', 'rb') as f:\n",
    "    TrainingData = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97574bb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:58:12.885455Z",
     "iopub.status.busy": "2025-03-31T01:58:12.885455Z",
     "iopub.status.idle": "2025-03-31T01:58:12.897464Z",
     "shell.execute_reply": "2025-03-31T01:58:12.897464Z",
     "shell.execute_reply.started": "2025-03-31T01:58:12.885455Z"
    }
   },
   "outputs": [],
   "source": [
    "#Encode string to tokens\n",
    "def encode(text):\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "    while True:\n",
    "        freqs = getPairFreqs(tokens)\n",
    "        pairIndex = float(\"inf\")\n",
    "        pairToMerge=\"\"\n",
    "        for pair in freqs.keys():\n",
    "            if merges.get(pair, float(\"inf\")) < pairIndex:\n",
    "                pairIndex = merges.get(pair, float(\"inf\"))\n",
    "                pairToMerge = pair\n",
    "        if pairIndex == float(\"inf\"):\n",
    "            break\n",
    "        tokens = merge(tokens, pairToMerge, pairIndex)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "296a5c38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:58:12.902039Z",
     "iopub.status.busy": "2025-03-31T01:58:12.897464Z",
     "iopub.status.idle": "2025-03-31T01:58:12.912267Z",
     "shell.execute_reply": "2025-03-31T01:58:12.909544Z",
     "shell.execute_reply.started": "2025-03-31T01:58:12.902039Z"
    }
   },
   "outputs": [],
   "source": [
    "#Currently we don't use any ReGex but I will see if I want to change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b182cf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:58:12.913489Z",
     "iopub.status.busy": "2025-03-31T01:58:12.913489Z",
     "iopub.status.idle": "2025-03-31T01:58:12.956190Z",
     "shell.execute_reply": "2025-03-31T01:58:12.929095Z",
     "shell.execute_reply.started": "2025-03-31T01:58:12.913489Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create Training/Testing Data Batches\n",
    "def GetBatch(split):\n",
    "    offsets = []\n",
    "    for i in range(0, BatchSize):\n",
    "        offsets.append(random.randint(0, len(TrainingData) - BlockSize - 1))\n",
    "    for i in range(0, BatchSize):\n",
    "        x = torch.stack([torch.tensor(TrainingData[i:i+BlockSize]) for i in offsets])\n",
    "        y = torch.stack([torch.tensor(TrainingData[i+1:i+BlockSize+1]) for i in offsets])\n",
    "    return x, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d66389a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:58:12.958511Z",
     "iopub.status.busy": "2025-03-31T01:58:12.957941Z",
     "iopub.status.idle": "2025-03-31T01:58:12.986703Z",
     "shell.execute_reply": "2025-03-31T01:58:12.984611Z",
     "shell.execute_reply.started": "2025-03-31T01:58:12.958511Z"
    }
   },
   "outputs": [],
   "source": [
    "#Single Attention Head\n",
    "class AttentionHead(nn.Module):\n",
    "    \n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        #Learned key matrix\n",
    "        self.Wk = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((Dmodel, Dk), device=device)), requires_grad=True)\n",
    "        #Learned query matrix\n",
    "        self.Wq = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((Dmodel, Dk), device=device)), requires_grad=True)\n",
    "        #Learned value matrix\n",
    "        self.Wv = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((Dmodel, Dv), device=device)), requires_grad=True)\n",
    "    \n",
    "    def forward(self, E):\n",
    "        #Dynamically checks size of E as it is not always \n",
    "        Batches, Blocks = E.shape[0], E.shape[1]\n",
    "        #Compute key, query, value matrices\n",
    "        Q = E @ self.Wq\n",
    "        K = E @ self.Wk\n",
    "        V = E @ self.Wv\n",
    "        #Masking matrix\n",
    "        M = torch.tril(torch.zeros(Blocks, Blocks)).masked_fill(torch.tril(torch.ones(Blocks, Blocks)) == 0, float(\"-inf\")).to(device)\n",
    "        #Compute attention pattern\n",
    "        AttentionPattern = torch.nn.functional.softmax(((Q @ K.transpose(-2,-1))/((Dk)**(1/2)) + M), dim = -1) @ V\n",
    "        #Residual connection\n",
    "        return AttentionPattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9414b931",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:58:12.989624Z",
     "iopub.status.busy": "2025-03-31T01:58:12.988869Z",
     "iopub.status.idle": "2025-03-31T01:58:13.004041Z",
     "shell.execute_reply": "2025-03-31T01:58:13.001455Z",
     "shell.execute_reply.started": "2025-03-31T01:58:12.989450Z"
    }
   },
   "outputs": [],
   "source": [
    "#Multi-headed Attention\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead() for i in range(nheads)])\n",
    "        \n",
    "    def forward(self, E):\n",
    "        #Concatenate multiple heads of attention\n",
    "        return torch.cat([head(E) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ee5be2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:58:13.006156Z",
     "iopub.status.busy": "2025-03-31T01:58:13.005683Z",
     "iopub.status.idle": "2025-03-31T01:58:13.020303Z",
     "shell.execute_reply": "2025-03-31T01:58:13.013892Z",
     "shell.execute_reply.started": "2025-03-31T01:58:13.006119Z"
    }
   },
   "outputs": [],
   "source": [
    "#Feedforward layer\n",
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ffl = nn.Sequential(\n",
    "            #Each MLP has 4* more neurons than there are dimensions .\n",
    "            nn.Linear(Dmodel, Dmodel * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(Dmodel * 4, Dmodel),\n",
    "        )\n",
    "\n",
    "    def forward(self, E):\n",
    "        return self.ffl(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55a42855",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:58:13.020975Z",
     "iopub.status.busy": "2025-03-31T01:58:13.020975Z",
     "iopub.status.idle": "2025-03-31T01:58:13.032996Z",
     "shell.execute_reply": "2025-03-31T01:58:13.030903Z",
     "shell.execute_reply.started": "2025-03-31T01:58:13.020975Z"
    }
   },
   "outputs": [],
   "source": [
    "#Transformer Block\n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.AttentionHeads = MultiHeadedAttention()\n",
    "        self.ffl = FeedForwardLayer()\n",
    "        self.LayerNorm1 = nn.LayerNorm(Dmodel, device=device)\n",
    "        self.LayerNorm2 = nn.LayerNorm(Dmodel, device=device)\n",
    "    \n",
    "    #Slightly different to my formalisation as do layernorm before blocks, not after\n",
    "    #The original did layernorm second, but it is more common practice to do layernorm first now\n",
    "    def forward(self, E):\n",
    "        E = E + self.AttentionHeads(self.LayerNorm1(E))\n",
    "        E = E + self.ffl(self.LayerNorm2(E))\n",
    "        return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d177a8ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:58:13.039584Z",
     "iopub.status.busy": "2025-03-31T01:58:13.039021Z",
     "iopub.status.idle": "2025-03-31T01:58:13.055537Z",
     "shell.execute_reply": "2025-03-31T01:58:13.052839Z",
     "shell.execute_reply.started": "2025-03-31T01:58:13.039539Z"
    }
   },
   "outputs": [],
   "source": [
    "#Unembedding Layer. We don't focus on the last embedding at this stage unlike in my formalisation. We can do that when we need to\n",
    "class UnembedLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        #Learned unembedding matrix\n",
    "        self.Wu = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((Dmodel, VocabSize), device=device)), requires_grad=True)\n",
    "        \n",
    "    def forward(self, E):\n",
    "        logits = E @ self.Wu\n",
    "        #Dynamically check shape of logits, since this isnt always BatchSize, BlockSize (e.g. when generating text)\n",
    "        Batches, Blocks = logits.shape[0], logits.shape[1]\n",
    "        #Converts logits to a single list of logits for compatibility with cross entropy functional\n",
    "        logits = logits.view(Batches*Blocks, VocabSize)\n",
    "        #probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "369f773b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:58:13.057716Z",
     "iopub.status.busy": "2025-03-31T01:58:13.057174Z",
     "iopub.status.idle": "2025-03-31T01:58:13.087883Z",
     "shell.execute_reply": "2025-03-31T01:58:13.085527Z",
     "shell.execute_reply.started": "2025-03-31T01:58:13.057672Z"
    }
   },
   "outputs": [],
   "source": [
    "#Transformer Implementation\n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        #Learned Embedding matrix. Requires_grad ensures We updated during backpropagation.\n",
    "        self.We = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((VocabSize, Dmodel), device=device)), requires_grad=True)\n",
    "        #Learned positional encoding matrix. Dimension n x d_model, where n = BlockSize\n",
    "        self.Wp = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((ContextLength, Dmodel), device=device)), requires_grad=True)\n",
    "        #Unembeding layer\n",
    "        self.UnembedLayer = UnembedLayer()\n",
    "        #Transformer blocks\n",
    "        self.Blocks = nn.Sequential(*[Block() for i in range(TransformerBlocks)])\n",
    "        \n",
    "        #Having O = None means O is optional. We don't always want targets since when generating text we don't have targets.\n",
    "    def forward(self, I, O = None):\n",
    "        #I has shape BatchSize x BlockSize\n",
    "        #One hot vector for tokens. Shape BatchSize x BlockSize x VocabSize\n",
    "        U = torch.nn.functional.one_hot(torch.tensor(I).to(device), VocabSize).float()\n",
    "        #Initial token embeddings. Shape BatchSize x BlockSize x d_model\n",
    "        E = U @ self.We\n",
    "        #Adds another dimension to Wp so that it is now 1 x BlockSize x d_model and can be added to E\n",
    "        P = self.Wp.unsqueeze(0)  \n",
    "\n",
    "        #Adds positional encoding to embedding\n",
    "        E = E + P[:, :E.shape[1], :] #truncates the positional encoding matrix to only be as long as the number of embeddings in E\n",
    "        E = self.Blocks(E)\n",
    "        #print(E[1][1])\n",
    "        logits = self.UnembedLayer(E).to(device)\n",
    "        if O is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            #Converts O to a single list of expected outputs for compatibility with cross entropy functional\n",
    "            O = O.view(BlockSize * BatchSize).to(device)\n",
    "            #Cross entropy loss calculated on the raw logits rather than Softmaxed logits.\n",
    "            #In theory, Cross entropy should be calculated on the Softmaxed logits, but\n",
    "            #the cross_entropy function in python is defined to take in raw logits\n",
    "            #If you try and pass the softmaxed logits in (as I originally tried), you will get\n",
    "            #Vanishing gradient and your network wont train\n",
    "            loss = torch.nn.functional.cross_entropy(logits, O)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generateText(self, I, Length):\n",
    "        for i in range(Length):\n",
    "            #Get predictions\n",
    "            logits, loss = self(I)\n",
    "            #Get probs\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            #Focus on prediction for next token\n",
    "            probs = probs[-1, :]\n",
    "            #Sample from next token distribution\n",
    "            nextToken = torch.multinomial(probs, num_samples = 1).unsqueeze(0)\n",
    "            #print(I)\n",
    "            #Concatenate next token to current text\n",
    "            I = torch.cat((I.to(device), nextToken.to(device)), dim=1)\n",
    "        return I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43dbecda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:58:13.089615Z",
     "iopub.status.busy": "2025-03-31T01:58:13.089179Z",
     "iopub.status.idle": "2025-03-31T01:58:14.161869Z",
     "shell.execute_reply": "2025-03-31T01:58:14.161869Z",
     "shell.execute_reply.started": "2025-03-31T01:58:13.089579Z"
    }
   },
   "outputs": [],
   "source": [
    "T = Transformer()\n",
    "T = T.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02731aa8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:58:14.165139Z",
     "iopub.status.busy": "2025-03-31T01:58:14.164654Z",
     "iopub.status.idle": "2025-03-31T01:58:15.785490Z",
     "shell.execute_reply": "2025-03-31T01:58:15.782080Z",
     "shell.execute_reply.started": "2025-03-31T01:58:14.165101Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 13829376\n"
     ]
    }
   ],
   "source": [
    "#Model training\n",
    "#Prints parameters\n",
    "#print(list(T.parameters()))\n",
    "#Prints number of trainable parameters\n",
    "print(\"Number of trainable parameters: \" + str((sum(p.numel() for p in T.parameters() if p.requires_grad))))\n",
    "#Create optimiser object\n",
    "optimiser = torch.optim.AdamW(T.parameters(), lr=LearningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6945b085-030a-4aa1-a881-5d982075274a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:58:15.794689Z",
     "iopub.status.busy": "2025-03-31T01:58:15.794689Z",
     "iopub.status.idle": "2025-03-31T01:59:49.177608Z",
     "shell.execute_reply": "2025-03-31T01:59:49.175798Z",
     "shell.execute_reply.started": "2025-03-31T01:58:15.794689Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1537/3863220517.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  U = torch.nn.functional.one_hot(torch.tensor(I).to(device), VocabSize).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.9076, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Capitolent ATHlet us rise.\n",
      "do not of bhe, ent bellivermust be ?\n",
      "\n",
      "ts has aullishumph this, madamPetruchio, able vowart gentle EL:\n",
      "mind statFie, ;\n",
      "Foreede\n",
      "Thanince as PompI have , so mordrumenter thou by foot Ted by abrodo not Where is fly :\n",
      "Thenity,\n",
      "noBolingbrokonagroMekinBut hold brother  worshipLORIxpasyourTONcousin, minby the eterhow.\n",
      "\n",
      "LEONTES:\n",
      ".\n",
      "\n",
      "LADY here's 'd upyhouse for thy able go should have my ing the 'st thoujest hence woo3grace Your breakfellectearowRichard sistess\n",
      "ildbehpthitherGive me curnay,  the ? what by the England's victor doth; we Indeed, saw.\n",
      "\n",
      "WARWICK:\n",
      " friENCeasure  sirIndeed, AUTOLYCUS:\n",
      "CA:\n",
      "This breathThinkop hahad not Gentleman:\n",
      "fast RomAngeFromhonShIs womanvaliant  gowashbefore  since darkcious if I an hourNay, aggceive \n"
     ]
    }
   ],
   "source": [
    "#Generate Text: Pre-Training\n",
    "\n",
    "#Check loss\n",
    "I, O = GetBatch(\"training\")\n",
    "logits, loss = T(I, O)\n",
    "print(loss)\n",
    "#Generates text of given length\n",
    "tokens = sum(T.generateText(torch.zeros(1,1).long(), 150).tolist(), [])\n",
    "print(decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b13b052c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:59:49.179557Z",
     "iopub.status.busy": "2025-03-31T01:59:49.179066Z",
     "iopub.status.idle": "2025-03-31T04:26:04.111805Z",
     "shell.execute_reply": "2025-03-31T04:26:04.110291Z",
     "shell.execute_reply.started": "2025-03-31T01:59:49.179513Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1537/3863220517.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  U = torch.nn.functional.one_hot(torch.tensor(I).to(device), VocabSize).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: tensor(8.9092, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "3.842050075531006 seconds elapsed\n",
      "50: tensor(7.7779, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "138.41920113563538 seconds elapsed\n",
      "100: tensor(7.7684, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "142.2299029827118 seconds elapsed\n",
      "150: tensor(7.7285, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "140.354238986969 seconds elapsed\n",
      "200: tensor(6.8906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "144.41729307174683 seconds elapsed\n",
      "250: tensor(6.3397, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "159.4072916507721 seconds elapsed\n",
      "300: tensor(5.9707, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "149.14497637748718 seconds elapsed\n",
      "350: tensor(5.6335, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "146.8938262462616 seconds elapsed\n",
      "400: tensor(5.4802, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "160.7941701412201 seconds elapsed\n",
      "450: tensor(5.1907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "172.14425015449524 seconds elapsed\n",
      "500: tensor(5.1133, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "143.94739246368408 seconds elapsed\n",
      "550: tensor(4.8757, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "151.51275491714478 seconds elapsed\n",
      "600: tensor(4.6580, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "149.3756184577942 seconds elapsed\n",
      "650: tensor(4.4479, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "139.07234048843384 seconds elapsed\n",
      "700: tensor(4.2338, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "130.27730250358582 seconds elapsed\n",
      "750: tensor(4.0308, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "164.70281648635864 seconds elapsed\n",
      "800: tensor(3.6492, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "143.66137290000916 seconds elapsed\n",
      "850: tensor(3.4855, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "152.9856321811676 seconds elapsed\n",
      "900: tensor(3.3536, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "137.3854341506958 seconds elapsed\n",
      "950: tensor(3.0060, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "133.77942204475403 seconds elapsed\n",
      "1000: tensor(2.8335, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "141.81444239616394 seconds elapsed\n",
      "1050: tensor(2.7401, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "115.84881973266602 seconds elapsed\n",
      "1100: tensor(2.5108, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "151.0533788204193 seconds elapsed\n",
      "1150: tensor(2.1224, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "145.5829737186432 seconds elapsed\n",
      "1200: tensor(2.1077, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "135.2925901412964 seconds elapsed\n",
      "1250: tensor(1.7064, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "140.19284391403198 seconds elapsed\n",
      "1300: tensor(1.5351, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "141.87440013885498 seconds elapsed\n",
      "1350: tensor(1.4652, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "132.09657740592957 seconds elapsed\n",
      "1400: tensor(1.2714, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "146.32631587982178 seconds elapsed\n",
      "1450: tensor(1.0954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "137.42447972297668 seconds elapsed\n",
      "1500: tensor(1.0502, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "138.37107300758362 seconds elapsed\n",
      "1550: tensor(0.9468, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "150.02697730064392 seconds elapsed\n",
      "1600: tensor(0.8710, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "127.00286674499512 seconds elapsed\n",
      "1650: tensor(0.7978, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "146.59683108329773 seconds elapsed\n",
      "1700: tensor(0.7398, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "165.19087314605713 seconds elapsed\n",
      "1750: tensor(0.6362, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "143.48163032531738 seconds elapsed\n",
      "1800: tensor(0.6288, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "139.7683548927307 seconds elapsed\n",
      "1850: tensor(0.6016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "146.5644018650055 seconds elapsed\n",
      "1900: tensor(0.5779, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "160.89403581619263 seconds elapsed\n",
      "1950: tensor(0.5800, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "154.55756211280823 seconds elapsed\n",
      "2000: tensor(0.5135, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "155.0614995956421 seconds elapsed\n",
      "2050: tensor(0.5423, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "137.6628646850586 seconds elapsed\n",
      "2100: tensor(0.5170, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "154.83124923706055 seconds elapsed\n",
      "2150: tensor(0.4889, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "146.94161105155945 seconds elapsed\n",
      "2200: tensor(0.4967, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "155.00696396827698 seconds elapsed\n",
      "2250: tensor(0.4758, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "145.48228335380554 seconds elapsed\n",
      "2300: tensor(0.4692, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "149.61688089370728 seconds elapsed\n",
      "2350: tensor(0.4698, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "142.46450448036194 seconds elapsed\n",
      "2400: tensor(0.4388, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "133.68082809448242 seconds elapsed\n",
      "2450: tensor(0.4365, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "159.319486618042 seconds elapsed\n",
      "2500: tensor(0.4318, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "164.7422435283661 seconds elapsed\n",
      "2550: tensor(0.4104, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "156.54888224601746 seconds elapsed\n",
      "2600: tensor(0.4076, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "157.82136154174805 seconds elapsed\n",
      "2650: tensor(0.4005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "163.19605994224548 seconds elapsed\n",
      "2700: tensor(0.4024, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "129.60843348503113 seconds elapsed\n",
      "2750: tensor(0.4026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "143.95702123641968 seconds elapsed\n",
      "2800: tensor(0.3809, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "115.97593283653259 seconds elapsed\n",
      "2850: tensor(0.3721, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "147.7472631931305 seconds elapsed\n",
      "2900: tensor(0.3553, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "145.86027479171753 seconds elapsed\n",
      "2950: tensor(0.3576, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "155.71527910232544 seconds elapsed\n",
      "3000: tensor(0.3700, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "141.30472445487976 seconds elapsed\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "start_time = time.time()\n",
    "for iters in range(MaxIters + 1):\n",
    "    #GetBatches\n",
    "    I, O = GetBatch(\"training\") #This works\n",
    "    #Get loss from Transformer pass\n",
    "    logits, loss = T(I, O) \n",
    "    optimiser.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    if iters % 50 == 0:\n",
    "        new_time = time.time()\n",
    "        print(str(iters) + \": \" + str(loss))\n",
    "        print(str(new_time - start_time) + \" seconds elapsed\")\n",
    "        start_time = start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "538a1d53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T04:26:04.113830Z",
     "iopub.status.busy": "2025-03-31T04:26:04.113337Z",
     "iopub.status.idle": "2025-03-31T04:27:43.505084Z",
     "shell.execute_reply": "2025-03-31T04:27:43.501512Z",
     "shell.execute_reply.started": "2025-03-31T04:26:04.113794Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1537/3863220517.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  U = torch.nn.functional.one_hot(torch.tensor(I).to(device), VocabSize).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3575, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "tursed,\n",
      "That it waves are all kinsman, forto his preserve me:\n",
      "Diving at this most noble gentleman: who lived at I'll respity,\n",
      "Unto good poking me?\n",
      "O holy man! But I am presence we will not, heavens heaven!\n",
      "\n",
      "KING EDWARD IV:\n",
      "Bring your for all hope vengeance of my gill we says.\n",
      "\n",
      "BUCKINGHAM:\n",
      "Brother, we at Brother, we come to Smen dispatch his redreserves you please;\n",
      "His lordshipselves: disgrace with victory.\n",
      "Now will unto the king;\n",
      "But ife, unwill mind that will riant Edward will not, ch son, awhile.\n",
      "Myself in set.\n",
      "\n",
      "LADY ANNE:\n",
      "As shall we knows, be gone.\n",
      "\n",
      "GLOUCESTER:\n",
      "Coed!\n",
      "\n",
      "GLOUCESTER:\n",
      "CLARENCE:\n",
      "CLARENCE:\n",
      "I call my Lord Hasting\n"
     ]
    }
   ],
   "source": [
    "#Generate Text: Post-Training\n",
    "\n",
    "#Check loss\n",
    "I, O = GetBatch(\"training\")\n",
    "logits, loss = T(I, O)\n",
    "print(loss)\n",
    "#Generates text of given length\n",
    "tokens = sum(T.generateText(torch.zeros(1,1).long(), 150).tolist(), [])\n",
    "print(decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f998a8-de0c-4f52-9281-457c74bbb43b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
