{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdcb72b-dc66-41f5-b535-bfa7fbc8e21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Tokeniser and Save to File\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c2cb81-4113-47e2-851a-6bf21618a402",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "PercentTraining = 1\n",
    "VocabSize = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba668efd-ba59-4985-940d-397b1230fda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load input text (Lorem Ipsum)\n",
    "with open(\"LoremIpsum.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc74347d-d5c4-4b92-ab32-f1b1ed181996",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load input text (Shakespeare)\n",
    "#with open(\"Tiny Shakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560e62a1-f6f6-412e-8e91-11142e8510af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load input text (XFM)\n",
    "#with open(\"XFM.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14efb405-15d5-4442-bdcd-8505c584663b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd03d7f-d996-416f-9d46-9cbf5d5ae613",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decode string from tokens\n",
    "def decode(tokens):\n",
    "    str = []\n",
    "    index = 0\n",
    "    offset = 0\n",
    "    while index - offset < len(tokens):\n",
    "        offset = 0\n",
    "        token = tokens[index]\n",
    "        #print(token)\n",
    "        if token in Initialvocab:\n",
    "            str.append(token)\n",
    "        else:\n",
    "            #print(merges)\n",
    "            if token in merges.values():\n",
    "                 for key, value in merges.items():\n",
    "                    if value == token:\n",
    "                        #print(token)\n",
    "                        tokens.insert(index + 1, key[0])\n",
    "                        tokens.insert(index + 2, key[1])\n",
    "                        offset = 1\n",
    "                        break\n",
    "        index +=1\n",
    "    return bytes(str).decode(\"utf-8\", errors=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b0c9bc-e613-4f61-a248-4a654206d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode text as bytes\n",
    "tokenisedText = list(map(int, text.encode(\"utf-8\")))\n",
    "#Split text into training and testing data\n",
    "n = int(PercentTraining*len(text))\n",
    "TrainingData = tokenisedText[:n]\n",
    "TestingData = tokenisedText[n:]\n",
    "print(\"Initial length: \" + str(len(TrainingData)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baafb16-5157-44a5-8547-28ba19451cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPairFreqs(text):\n",
    "    freqs = {}\n",
    "    for pair in zip(text, text[1:]):\n",
    "        try:\n",
    "            freqs[pair] +=1\n",
    "        except KeyError:\n",
    "            freqs[pair] = 1\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc76211-0817-4c78-9e29-abcdce7e0352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(text, pair, newChar):\n",
    "    newText = []\n",
    "    i=0\n",
    "    while i < len(text):\n",
    "        if i < len(text) - 1 and (text[i], text[i+1]) == (pair[0], pair[1]):\n",
    "            newText.append(newChar)\n",
    "            i+=2\n",
    "        else:\n",
    "            newText.append(text[i])\n",
    "            i+=1\n",
    "    return newText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1245646-67a7-4b3f-83ca-c2648a380954",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the tokeniser using BPE.\n",
    "start_time = time.time()\n",
    "Initialvocab = sorted(list(set(TrainingData)))\n",
    "#We remove the characters that aren't UTF-8 compatible from initialVocabulary before tokeniser training\n",
    "for word in Initialvocab[:]:\n",
    "    try:\n",
    "        bytes([word]).decode(\"utf-8\")\n",
    "    except:\n",
    "        Initialvocab.remove(word)\n",
    "vocab = Initialvocab.copy()\n",
    "#print(vocab)\n",
    "newChar = max(vocab) + 1\n",
    "merges = {}\n",
    "while len(vocab) < VocabSize:\n",
    "    freqs = getPairFreqs(TrainingData)\n",
    "    topPair = max(freqs, key=freqs.get)\n",
    "    TrainingData = merge(TrainingData, topPair, newChar)\n",
    "    vocab.append(newChar)\n",
    "    merges[topPair] = newChar\n",
    "    newChar +=1\n",
    "    if len(vocab) % 25 == 0:\n",
    "        new_time = time.time()\n",
    "        print(len(vocab))\n",
    "        print(str(new_time - start_time) + \" seconds elapsed\")\n",
    "        start_time = start_time = time.time()\n",
    "#My code is inefficient in the sense that there are gaps in the vocabulary e.g. we might have token \"10\" and then token \"32\",\n",
    "#i.e. no tokens are indexed between 10 and 32. Whilst the number of tokens in the Vocabulary is still VocabSize as I defined it,\n",
    "#We need to update VocabSize to be the highest numbered token in the vocabulary, so that we can define our one hot encoding correctly.\n",
    "#This does create some redundancy in entries of our matrices, which is inefficient and I would have addressed if I had more time.\n",
    "#However as VocabSize gets large, these redunancies become a smaller % of the VocabSize as we aren't introducing any new redundancies\n",
    "#through the merging process\n",
    "VocabSize = newChar\n",
    "print(\"Tokenised length: \" + str(len(TrainingData)))\n",
    "print(decode(vocab))\n",
    "\n",
    "# Save the variables using pickle so that I only need to train the tokeniser once\n",
    "with open('initialvocab.pkl', 'wb') as f:\n",
    "    pickle.dump(Initialvocab, f)\n",
    "\n",
    "with open('vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "\n",
    "with open('vocabsize.pkl', 'wb') as f:\n",
    "    pickle.dump(VocabSize, f)\n",
    "\n",
    "with open('merges.pkl', 'wb') as f:\n",
    "    pickle.dump(merges, f)\n",
    "\n",
    "with open('trainingdata.pkl', 'wb') as f:\n",
    "    pickle.dump(TrainingData, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
