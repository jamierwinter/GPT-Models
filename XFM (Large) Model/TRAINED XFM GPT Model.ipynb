{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01e9c164",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T06:08:31.207704Z",
     "iopub.status.busy": "2025-03-31T06:08:31.207188Z",
     "iopub.status.idle": "2025-03-31T06:08:35.994148Z",
     "shell.execute_reply": "2025-03-31T06:08:35.987275Z",
     "shell.execute_reply.started": "2025-03-31T06:08:31.207668Z"
    }
   },
   "outputs": [],
   "source": [
    "#Model to generate Lorem Ipsum\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "612399b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T06:08:36.010135Z",
     "iopub.status.busy": "2025-03-31T06:08:36.008195Z",
     "iopub.status.idle": "2025-03-31T06:08:36.127157Z",
     "shell.execute_reply": "2025-03-31T06:08:36.125943Z",
     "shell.execute_reply.started": "2025-03-31T06:08:36.010135Z"
    }
   },
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "BatchSize = 128\n",
    "BlockSize = 256\n",
    "Dmodel = 512\n",
    "nheads = 8\n",
    "Dk = int(Dmodel/nheads)\n",
    "Dv = Dk\n",
    "LearningRate = 2e-4\n",
    "MaxIters = 5000\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ContextLength=500\n",
    "TransformerBlocks = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d39483cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T06:08:36.127246Z",
     "iopub.status.busy": "2025-03-31T06:08:36.127157Z",
     "iopub.status.idle": "2025-03-31T06:08:36.206867Z",
     "shell.execute_reply": "2025-03-31T06:08:36.143559Z",
     "shell.execute_reply.started": "2025-03-31T06:08:36.127211Z"
    }
   },
   "outputs": [],
   "source": [
    "#Decode string from tokens\n",
    "def decode(tokens):\n",
    "    str = []\n",
    "    index = 0\n",
    "    offset = 0\n",
    "    while index - offset < len(tokens):\n",
    "        offset = 0\n",
    "        token = tokens[index]\n",
    "        #print(token)\n",
    "        if token in Initialvocab:\n",
    "            str.append(token)\n",
    "        else:\n",
    "            #print(merges)\n",
    "            if token in merges.values():\n",
    "                 for key, value in merges.items():\n",
    "                    if value == token:\n",
    "                        #print(token)\n",
    "                        tokens.insert(index + 1, key[0])\n",
    "                        tokens.insert(index + 2, key[1])\n",
    "                        offset = 1\n",
    "                        break\n",
    "        index +=1\n",
    "    return bytes(str).decode(\"utf-8\", errors=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58c427f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T06:08:36.216757Z",
     "iopub.status.busy": "2025-03-31T06:08:36.216214Z",
     "iopub.status.idle": "2025-03-31T06:08:36.228247Z",
     "shell.execute_reply": "2025-03-31T06:08:36.226604Z",
     "shell.execute_reply.started": "2025-03-31T06:08:36.216715Z"
    }
   },
   "outputs": [],
   "source": [
    "def getPairFreqs(text):\n",
    "    freqs = {}\n",
    "    for pair in zip(text, text[1:]):\n",
    "        try:\n",
    "            freqs[pair] +=1\n",
    "        except KeyError:\n",
    "            freqs[pair] = 1\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f3c5cbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T06:08:36.232498Z",
     "iopub.status.busy": "2025-03-31T06:08:36.232040Z",
     "iopub.status.idle": "2025-03-31T06:08:36.242789Z",
     "shell.execute_reply": "2025-03-31T06:08:36.236380Z",
     "shell.execute_reply.started": "2025-03-31T06:08:36.232498Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge(text, pair, newChar):\n",
    "    newText = []\n",
    "    i=0\n",
    "    while i < len(text):\n",
    "        if i < len(text) - 1 and (text[i], text[i+1]) == (pair[0], pair[1]):\n",
    "            newText.append(newChar)\n",
    "            i+=2\n",
    "        else:\n",
    "            newText.append(text[i])\n",
    "            i+=1\n",
    "    return newText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c7d4039",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T06:08:36.245850Z",
     "iopub.status.busy": "2025-03-31T06:08:36.245184Z",
     "iopub.status.idle": "2025-03-31T06:08:36.629471Z",
     "shell.execute_reply": "2025-03-31T06:08:36.629471Z",
     "shell.execute_reply.started": "2025-03-31T06:08:36.245850Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load variables from pickle files (Lorem Ipsum)\n",
    "with open('XFMinitialvocab.pkl', 'rb') as f:\n",
    "    Initialvocab = pickle.load(f)\n",
    "    \n",
    "with open('XFMvocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "    \n",
    "with open('XFMvocabsize.pkl', 'rb') as f:\n",
    "    VocabSize = pickle.load(f)\n",
    "    \n",
    "with open('XFMmerges.pkl', 'rb') as f:\n",
    "    merges = pickle.load(f)\n",
    "    \n",
    "with open('XFMtrainingdata.pkl', 'rb') as f:\n",
    "    TrainingData = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97574bb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T06:08:36.633459Z",
     "iopub.status.busy": "2025-03-31T06:08:36.633459Z",
     "iopub.status.idle": "2025-03-31T06:08:36.647068Z",
     "shell.execute_reply": "2025-03-31T06:08:36.645677Z",
     "shell.execute_reply.started": "2025-03-31T06:08:36.633459Z"
    }
   },
   "outputs": [],
   "source": [
    "#Encode string to tokens\n",
    "def encode(text):\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "    while True:\n",
    "        freqs = getPairFreqs(tokens)\n",
    "        pairIndex = float(\"inf\")\n",
    "        pairToMerge=\"\"\n",
    "        for pair in freqs.keys():\n",
    "            if merges.get(pair, float(\"inf\")) < pairIndex:\n",
    "                pairIndex = merges.get(pair, float(\"inf\"))\n",
    "                pairToMerge = pair\n",
    "        if pairIndex == float(\"inf\"):\n",
    "            break\n",
    "        tokens = merge(tokens, pairToMerge, pairIndex)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "296a5c38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T06:08:36.649481Z",
     "iopub.status.busy": "2025-03-31T06:08:36.649481Z",
     "iopub.status.idle": "2025-03-31T06:08:36.661462Z",
     "shell.execute_reply": "2025-03-31T06:08:36.659364Z",
     "shell.execute_reply.started": "2025-03-31T06:08:36.649481Z"
    }
   },
   "outputs": [],
   "source": [
    "#Currently we don't use any ReGex but I will see if I want to change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b182cf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T06:08:36.661462Z",
     "iopub.status.busy": "2025-03-31T06:08:36.661462Z",
     "iopub.status.idle": "2025-03-31T06:08:36.674963Z",
     "shell.execute_reply": "2025-03-31T06:08:36.673461Z",
     "shell.execute_reply.started": "2025-03-31T06:08:36.661462Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create Training/Testing Data Batches\n",
    "def GetBatch(split):\n",
    "    offsets = []\n",
    "    for i in range(0, BatchSize):\n",
    "        offsets.append(random.randint(0, len(TrainingData) - BlockSize - 1))\n",
    "    for i in range(0, BatchSize):\n",
    "        x = torch.stack([torch.tensor(TrainingData[i:i+BlockSize]) for i in offsets])\n",
    "        y = torch.stack([torch.tensor(TrainingData[i+1:i+BlockSize+1]) for i in offsets])\n",
    "    return x, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d66389a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T06:08:36.677460Z",
     "iopub.status.busy": "2025-03-31T06:08:36.676843Z",
     "iopub.status.idle": "2025-03-31T06:08:36.694683Z",
     "shell.execute_reply": "2025-03-31T06:08:36.694050Z",
     "shell.execute_reply.started": "2025-03-31T06:08:36.677460Z"
    }
   },
   "outputs": [],
   "source": [
    "#Single Attention Head\n",
    "class AttentionHead(nn.Module):\n",
    "    \n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        #Learned key matrix\n",
    "        self.Wk = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((Dmodel, Dk), device=device)), requires_grad=True)\n",
    "        #Learned query matrix\n",
    "        self.Wq = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((Dmodel, Dk), device=device)), requires_grad=True)\n",
    "        #Learned value matrix\n",
    "        self.Wv = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((Dmodel, Dv), device=device)), requires_grad=True)\n",
    "    \n",
    "    def forward(self, E):\n",
    "        #Dynamically checks size of E as it is not always \n",
    "        Batches, Blocks = E.shape[0], E.shape[1]\n",
    "        #Compute key, query, value matrices\n",
    "        Q = E @ self.Wq\n",
    "        K = E @ self.Wk\n",
    "        V = E @ self.Wv\n",
    "        #Masking matrix\n",
    "        M = torch.tril(torch.zeros(Blocks, Blocks)).masked_fill(torch.tril(torch.ones(Blocks, Blocks)) == 0, float(\"-inf\")).to(device)\n",
    "        #Compute attention pattern\n",
    "        AttentionPattern = torch.nn.functional.softmax(((Q @ K.transpose(-2,-1))/((Dk)**(1/2)) + M), dim = -1) @ V\n",
    "        #Residual connection\n",
    "        return AttentionPattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9414b931",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T06:08:36.697455Z",
     "iopub.status.busy": "2025-03-31T06:08:36.697455Z",
     "iopub.status.idle": "2025-03-31T06:08:36.745462Z",
     "shell.execute_reply": "2025-03-31T06:08:36.741461Z",
     "shell.execute_reply.started": "2025-03-31T06:08:36.697455Z"
    }
   },
   "outputs": [],
   "source": [
    "#Multi-headed Attention\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead() for i in range(nheads)])\n",
    "        \n",
    "    def forward(self, E):\n",
    "        #Concatenate multiple heads of attention\n",
    "        return torch.cat([head(E) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ee5be2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T06:08:36.745462Z",
     "iopub.status.busy": "2025-03-31T06:08:36.745462Z",
     "iopub.status.idle": "2025-03-31T06:08:36.761457Z",
     "shell.execute_reply": "2025-03-31T06:08:36.759598Z",
     "shell.execute_reply.started": "2025-03-31T06:08:36.745462Z"
    }
   },
   "outputs": [],
   "source": [
    "#Feedforward layer\n",
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ffl = nn.Sequential(\n",
    "            #Each MLP has 4* more neurons than there are dimensions .\n",
    "            nn.Linear(Dmodel, Dmodel * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(Dmodel * 4, Dmodel),\n",
    "        )\n",
    "\n",
    "    def forward(self, E):\n",
    "        return self.ffl(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55a42855",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T06:08:36.763268Z",
     "iopub.status.busy": "2025-03-31T06:08:36.763268Z",
     "iopub.status.idle": "2025-03-31T06:08:36.778006Z",
     "shell.execute_reply": "2025-03-31T06:08:36.775802Z",
     "shell.execute_reply.started": "2025-03-31T06:08:36.763268Z"
    }
   },
   "outputs": [],
   "source": [
    "#Transformer Block\n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.AttentionHeads = MultiHeadedAttention()\n",
    "        self.ffl = FeedForwardLayer()\n",
    "        self.LayerNorm1 = nn.LayerNorm(Dmodel, device=device)\n",
    "        self.LayerNorm2 = nn.LayerNorm(Dmodel, device=device)\n",
    "    \n",
    "    #Slightly different to my formalisation as do layernorm before blocks, not after\n",
    "    #The original did layernorm second, but it is more common practice to do layernorm first now\n",
    "    def forward(self, E):\n",
    "        E = E + self.AttentionHeads(self.LayerNorm1(E))\n",
    "        E = E + self.ffl(self.LayerNorm2(E))\n",
    "        return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d177a8ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T06:08:36.779976Z",
     "iopub.status.busy": "2025-03-31T06:08:36.779085Z",
     "iopub.status.idle": "2025-03-31T06:08:36.854472Z",
     "shell.execute_reply": "2025-03-31T06:08:36.848329Z",
     "shell.execute_reply.started": "2025-03-31T06:08:36.779899Z"
    }
   },
   "outputs": [],
   "source": [
    "#Unembedding Layer. We don't focus on the last embedding at this stage unlike in my formalisation. We can do that when we need to\n",
    "class UnembedLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        #Learned unembedding matrix\n",
    "        self.Wu = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((Dmodel, VocabSize), device=device)), requires_grad=True)\n",
    "        \n",
    "    def forward(self, E):\n",
    "        logits = E @ self.Wu\n",
    "        #Dynamically check shape of logits, since this isnt always BatchSize, BlockSize (e.g. when generating text)\n",
    "        Batches, Blocks = logits.shape[0], logits.shape[1]\n",
    "        #Converts logits to a single list of logits for compatibility with cross entropy functional\n",
    "        logits = logits.view(Batches*Blocks, VocabSize)\n",
    "        #probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "369f773b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T06:08:36.857740Z",
     "iopub.status.busy": "2025-03-31T06:08:36.857131Z",
     "iopub.status.idle": "2025-03-31T06:08:36.871875Z",
     "shell.execute_reply": "2025-03-31T06:08:36.869969Z",
     "shell.execute_reply.started": "2025-03-31T06:08:36.857686Z"
    }
   },
   "outputs": [],
   "source": [
    "#Transformer Implementation\n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        #Learned Embedding matrix. Requires_grad ensures We updated during backpropagation.\n",
    "        self.We = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((VocabSize, Dmodel), device=device)), requires_grad=True)\n",
    "        #Learned positional encoding matrix. Dimension n x d_model, where n = BlockSize\n",
    "        self.Wp = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((ContextLength, Dmodel), device=device)), requires_grad=True)\n",
    "        #Unembeding layer\n",
    "        self.UnembedLayer = UnembedLayer()\n",
    "        #Transformer blocks\n",
    "        self.Blocks = nn.Sequential(*[Block() for i in range(TransformerBlocks)])\n",
    "        \n",
    "        #Having O = None means O is optional. We don't always want targets since when generating text we don't have targets.\n",
    "    def forward(self, I, O = None):\n",
    "        #I has shape BatchSize x BlockSize\n",
    "        #One hot vector for tokens. Shape BatchSize x BlockSize x VocabSize\n",
    "        U = torch.nn.functional.one_hot(torch.tensor(I).to(device), VocabSize).float()\n",
    "        #Initial token embeddings. Shape BatchSize x BlockSize x d_model\n",
    "        E = U @ self.We\n",
    "        #Adds another dimension to Wp so that it is now 1 x BlockSize x d_model and can be added to E\n",
    "        P = self.Wp.unsqueeze(0)  \n",
    "\n",
    "        #Adds positional encoding to embedding\n",
    "        E = E + P[:, :E.shape[1], :] #truncates the positional encoding matrix to only be as long as the number of embeddings in E\n",
    "        E = self.Blocks(E)\n",
    "        #print(E[1][1])\n",
    "        logits = self.UnembedLayer(E).to(device)\n",
    "        if O is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            #Converts O to a single list of expected outputs for compatibility with cross entropy functional\n",
    "            O = O.view(BlockSize * BatchSize).to(device)\n",
    "            #Cross entropy loss calculated on the raw logits rather than Softmaxed logits.\n",
    "            #In theory, Cross entropy should be calculated on the Softmaxed logits, but\n",
    "            #the cross_entropy function in python is defined to take in raw logits\n",
    "            #If you try and pass the softmaxed logits in (as I originally tried), you will get\n",
    "            #Vanishing gradient and your network wont train\n",
    "            loss = torch.nn.functional.cross_entropy(logits, O)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generateText(self, I, Length):\n",
    "        for i in range(Length):\n",
    "            #Get predictions\n",
    "            logits, loss = self(I)\n",
    "            #Get probs\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            #Focus on prediction for next token\n",
    "            probs = probs[-1, :]\n",
    "            #Sample from next token distribution\n",
    "            nextToken = torch.multinomial(probs, num_samples = 1).unsqueeze(0)\n",
    "            #print(I)\n",
    "            #Concatenate next token to current text\n",
    "            I = torch.cat((I.to(device), nextToken.to(device)), dim=1)\n",
    "        return I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43dbecda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T06:08:36.874528Z",
     "iopub.status.busy": "2025-03-31T06:08:36.874120Z",
     "iopub.status.idle": "2025-03-31T06:08:37.978159Z",
     "shell.execute_reply": "2025-03-31T06:08:37.976121Z",
     "shell.execute_reply.started": "2025-03-31T06:08:36.874528Z"
    }
   },
   "outputs": [],
   "source": [
    "T = Transformer()\n",
    "T = T.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02731aa8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T06:08:37.980766Z",
     "iopub.status.busy": "2025-03-31T06:08:37.980262Z",
     "iopub.status.idle": "2025-03-31T06:08:39.762989Z",
     "shell.execute_reply": "2025-03-31T06:08:39.757580Z",
     "shell.execute_reply.started": "2025-03-31T06:08:37.980725Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 33640448\n"
     ]
    }
   ],
   "source": [
    "#Model training\n",
    "#Prints parameters\n",
    "#print(list(T.parameters()))\n",
    "#Prints number of trainable parameters\n",
    "print(\"Number of trainable parameters: \" + str((sum(p.numel() for p in T.parameters() if p.requires_grad))))\n",
    "#Create optimiser object\n",
    "optimiser = torch.optim.AdamW(T.parameters(), lr=LearningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6945b085-030a-4aa1-a881-5d982075274a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T06:08:39.773358Z",
     "iopub.status.busy": "2025-03-31T06:08:39.769519Z",
     "iopub.status.idle": "2025-03-31T06:10:50.672402Z",
     "shell.execute_reply": "2025-03-31T06:10:50.670679Z",
     "shell.execute_reply.started": "2025-03-31T06:08:39.773358Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2122/3863220517.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  U = torch.nn.functional.one_hot(torch.tensor(I).to(device), VocabSize).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.5638, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "towards antenuequit's not a dangeroustogepool summatcca banansay the down to ing aboutsomeone elseke, ...\"\n",
      "karl: people are or something gone poor...\n",
      "steve: That's a presadvices, righttufta: wondered 9mumdown and you put .\n",
      "unknown c: Yeahdoes frewhat he child sharwhereas iteverywherepresentlaughs safmobistraight born avaiintromediEusome Laughs\n",
      "steve: switchwaxJohn you don't know seRight, \n",
      "steve: SI don't . My lieconsthead move onRed evil enjoyed difficulnicking raising, and shouldn't be Lyeresaid 'available ues. So, so . They're all attoo. Karl  three dinnersolute no-ps [inaudibleSteve Merchanswatelly, MerreWell, Robinueswith meused to pasttedexactly lock?\n",
      "unknown a: Well, syntell me because I was when it cool that's the Viwhoin their ok, ... But .\n",
      "karl: He Hello makes a a crash helmetbloke knowndid youpants ity, team being wifnamgovernToo put them syn. Surehospital Oh! whatchat do that Hosongchocolat. Everyone's can't, waterWonders-watchpoint in a lotworld \n"
     ]
    }
   ],
   "source": [
    "#Generate Text: Pre-Training\n",
    "\n",
    "#Check loss\n",
    "I, O = GetBatch(\"training\")\n",
    "logits, loss = T(I, O)\n",
    "print(loss)\n",
    "#Generates text of given length\n",
    "tokens = sum(T.generateText(torch.zeros(1,1).long(), 150).tolist(), [])\n",
    "print(decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b13b052c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T06:10:50.675133Z",
     "iopub.status.busy": "2025-03-31T06:10:50.674510Z",
     "iopub.status.idle": "2025-03-31T17:36:31.940101Z",
     "shell.execute_reply": "2025-03-31T17:36:31.920718Z",
     "shell.execute_reply.started": "2025-03-31T06:10:50.675091Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2122/3863220517.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  U = torch.nn.functional.one_hot(torch.tensor(I).to(device), VocabSize).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: tensor(9.5675, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "9.358599185943604 seconds elapsed\n",
      "50: tensor(8.4305, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "410.10340642929077 seconds elapsed\n",
      "100: tensor(8.3874, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "416.59191060066223 seconds elapsed\n",
      "150: tensor(8.3434, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "417.83711099624634 seconds elapsed\n",
      "200: tensor(8.1751, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "398.81189584732056 seconds elapsed\n",
      "250: tensor(7.3860, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "422.4663758277893 seconds elapsed\n",
      "300: tensor(6.5260, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "390.5383381843567 seconds elapsed\n",
      "350: tensor(6.0284, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "427.9792423248291 seconds elapsed\n",
      "400: tensor(5.5838, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "397.0500133037567 seconds elapsed\n",
      "450: tensor(5.2302, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "413.83710074424744 seconds elapsed\n",
      "500: tensor(4.7247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "402.58006501197815 seconds elapsed\n",
      "550: tensor(4.4817, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "416.1486690044403 seconds elapsed\n",
      "600: tensor(4.1821, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "407.41715455055237 seconds elapsed\n",
      "650: tensor(3.8162, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "397.7460341453552 seconds elapsed\n",
      "700: tensor(3.5499, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "400.1449890136719 seconds elapsed\n",
      "750: tensor(3.2437, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "421.8904707431793 seconds elapsed\n",
      "800: tensor(2.8956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "405.88202023506165 seconds elapsed\n",
      "850: tensor(2.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "403.4645981788635 seconds elapsed\n",
      "900: tensor(2.4664, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "408.72276043891907 seconds elapsed\n",
      "950: tensor(2.2969, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "414.2039747238159 seconds elapsed\n",
      "1000: tensor(1.9928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "392.9443018436432 seconds elapsed\n",
      "1050: tensor(1.7559, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "395.4722774028778 seconds elapsed\n",
      "1100: tensor(1.6343, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "399.5645716190338 seconds elapsed\n",
      "1150: tensor(1.4316, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "419.02758717536926 seconds elapsed\n",
      "1200: tensor(1.2633, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "406.9576315879822 seconds elapsed\n",
      "1250: tensor(1.1665, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "418.48077273368835 seconds elapsed\n",
      "1300: tensor(1.0249, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "416.8158745765686 seconds elapsed\n",
      "1350: tensor(0.8883, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "424.5458927154541 seconds elapsed\n",
      "1400: tensor(0.8027, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "416.99751925468445 seconds elapsed\n",
      "1450: tensor(0.7371, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "409.75683641433716 seconds elapsed\n",
      "1500: tensor(0.7294, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "406.1059536933899 seconds elapsed\n",
      "1550: tensor(0.6515, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "428.8547854423523 seconds elapsed\n",
      "1600: tensor(0.5909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "417.8156518936157 seconds elapsed\n",
      "1650: tensor(0.5813, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "417.9277698993683 seconds elapsed\n",
      "1700: tensor(0.5372, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "408.8771233558655 seconds elapsed\n",
      "1750: tensor(0.5201, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "414.47162675857544 seconds elapsed\n",
      "1800: tensor(0.4786, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "407.30883979797363 seconds elapsed\n",
      "1850: tensor(0.4570, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "411.5188834667206 seconds elapsed\n",
      "1900: tensor(0.4346, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "418.44365191459656 seconds elapsed\n",
      "1950: tensor(0.4251, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "397.50011110305786 seconds elapsed\n",
      "2000: tensor(0.4215, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "412.7637755870819 seconds elapsed\n",
      "2050: tensor(0.3940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "409.738627910614 seconds elapsed\n",
      "2100: tensor(0.3891, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "422.3137102127075 seconds elapsed\n",
      "2150: tensor(0.3807, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "413.71110463142395 seconds elapsed\n",
      "2200: tensor(0.3694, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "397.17424392700195 seconds elapsed\n",
      "2250: tensor(0.3618, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "416.23914980888367 seconds elapsed\n",
      "2300: tensor(0.3543, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "401.88185358047485 seconds elapsed\n",
      "2350: tensor(0.3406, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "408.1545367240906 seconds elapsed\n",
      "2400: tensor(0.3368, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "414.6943955421448 seconds elapsed\n",
      "2450: tensor(0.3203, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "424.8740622997284 seconds elapsed\n",
      "2500: tensor(0.3237, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "410.46717858314514 seconds elapsed\n",
      "2550: tensor(0.3129, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "399.3859431743622 seconds elapsed\n",
      "2600: tensor(0.3018, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "431.71048617362976 seconds elapsed\n",
      "2650: tensor(0.2986, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "379.9487886428833 seconds elapsed\n",
      "2700: tensor(0.3020, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "397.9444229602814 seconds elapsed\n",
      "2750: tensor(0.3062, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "394.1195557117462 seconds elapsed\n",
      "2800: tensor(0.2852, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "377.42491579055786 seconds elapsed\n",
      "2850: tensor(0.2855, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "400.11494994163513 seconds elapsed\n",
      "2900: tensor(0.2700, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "419.8254497051239 seconds elapsed\n",
      "2950: tensor(0.2692, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "403.8345992565155 seconds elapsed\n",
      "3000: tensor(0.2672, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "421.8289601802826 seconds elapsed\n",
      "3050: tensor(0.2561, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "410.6796658039093 seconds elapsed\n",
      "3100: tensor(0.2558, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "423.7684168815613 seconds elapsed\n",
      "3150: tensor(0.2506, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "415.7422935962677 seconds elapsed\n",
      "3200: tensor(0.2509, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "420.01757287979126 seconds elapsed\n",
      "3250: tensor(0.2409, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "396.61829018592834 seconds elapsed\n",
      "3300: tensor(0.2380, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "420.5985507965088 seconds elapsed\n",
      "3350: tensor(0.2273, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "412.93081545829773 seconds elapsed\n",
      "3400: tensor(0.2371, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "409.8869631290436 seconds elapsed\n",
      "3450: tensor(0.2467, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "427.64030027389526 seconds elapsed\n",
      "3500: tensor(0.2248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "419.71741223335266 seconds elapsed\n",
      "3550: tensor(0.2269, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "422.9093990325928 seconds elapsed\n",
      "3600: tensor(0.2114, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "418.60213708877563 seconds elapsed\n",
      "3650: tensor(0.2186, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "407.9074487686157 seconds elapsed\n",
      "3700: tensor(0.2113, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "419.075076341629 seconds elapsed\n",
      "3750: tensor(0.2146, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "414.85173416137695 seconds elapsed\n",
      "3800: tensor(0.2126, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "421.31957602500916 seconds elapsed\n",
      "3850: tensor(0.2054, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "422.2939889431 seconds elapsed\n",
      "3900: tensor(0.2009, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "403.3557872772217 seconds elapsed\n",
      "3950: tensor(0.2021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "417.4357533454895 seconds elapsed\n",
      "4000: tensor(0.2048, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "426.1933877468109 seconds elapsed\n",
      "4050: tensor(0.1893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "396.92454171180725 seconds elapsed\n",
      "4100: tensor(0.1899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "408.77010321617126 seconds elapsed\n",
      "4150: tensor(0.1923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "418.78961181640625 seconds elapsed\n",
      "4200: tensor(0.1962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "400.81218099594116 seconds elapsed\n",
      "4250: tensor(0.1888, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "420.2862310409546 seconds elapsed\n",
      "4300: tensor(0.1875, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "413.9508068561554 seconds elapsed\n",
      "4350: tensor(0.1782, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "394.18649435043335 seconds elapsed\n",
      "4400: tensor(0.1826, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "407.7992765903473 seconds elapsed\n",
      "4450: tensor(0.1784, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "412.6625943183899 seconds elapsed\n",
      "4500: tensor(0.1722, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "407.1047737598419 seconds elapsed\n",
      "4550: tensor(0.1752, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "424.0430910587311 seconds elapsed\n",
      "4600: tensor(0.1830, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "404.4488000869751 seconds elapsed\n",
      "4650: tensor(0.1679, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "386.73140954971313 seconds elapsed\n",
      "4700: tensor(0.1730, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "425.0538339614868 seconds elapsed\n",
      "4750: tensor(0.1710, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "386.8923079967499 seconds elapsed\n",
      "4800: tensor(0.1583, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "415.85753774642944 seconds elapsed\n",
      "4850: tensor(0.1565, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "414.7661066055298 seconds elapsed\n",
      "4900: tensor(0.1568, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "430.686194896698 seconds elapsed\n",
      "4950: tensor(0.1630, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "406.63112926483154 seconds elapsed\n",
      "5000: tensor(0.1636, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "405.7723171710968 seconds elapsed\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "start_time = time.time()\n",
    "for iters in range(MaxIters + 1):\n",
    "    #GetBatches\n",
    "    I, O = GetBatch(\"training\") #This works\n",
    "    #Get loss from Transformer pass\n",
    "    logits, loss = T(I, O) \n",
    "    optimiser.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    if iters % 50 == 0:\n",
    "        new_time = time.time()\n",
    "        print(str(iters) + \": \" + str(loss))\n",
    "        print(str(new_time - start_time) + \" seconds elapsed\")\n",
    "        start_time = start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "538a1d53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T17:36:31.950231Z",
     "iopub.status.busy": "2025-03-31T17:36:31.950231Z",
     "iopub.status.idle": "2025-03-31T17:38:53.753766Z",
     "shell.execute_reply": "2025-03-31T17:38:53.744040Z",
     "shell.execute_reply.started": "2025-03-31T17:36:31.950231Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2122/3863220517.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  U = torch.nn.functional.one_hot(torch.tensor(I).to(device), VocabSize).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1674, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "she?\n",
      "ricky: \"Yeah it's my film just because I don't feel light's got a month views I used to wait for that, he's doing in the morning to sort of election thing. Karl, did you do, do praise for quite a nice nights do- these things got after deaths he can have terrible.\n",
      "none: Ricky hootes\n",
      "ricky: If you see is he? I know, I don't like this name? And praise other period, stayed that out and out there. Don't know his eye for matches for a job. OhhDidn't happen.\n",
      "ricky: He probably wanted it.\n",
      "ricky: Okay. Did you want to show you think, it was like a bit of an exotic time ago were going to a raconscious days everyone else was doing this.\n",
      "ricky: No, this is not the real full's alarm was the detail off some of the fuan assess Rick.\n",
      "none: Ricky and Steve high againwaff\n",
      "steve: He's out on a minute and have called Shi\n"
     ]
    }
   ],
   "source": [
    "#Generate Text: Post-Training\n",
    "\n",
    "#Check loss\n",
    "I, O = GetBatch(\"training\")\n",
    "logits, loss = T(I, O)\n",
    "print(loss)\n",
    "#Generates text of given length\n",
    "tokens = sum(T.generateText(torch.zeros(1,1).long(), 150).tolist(), [])\n",
    "print(decode(tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
