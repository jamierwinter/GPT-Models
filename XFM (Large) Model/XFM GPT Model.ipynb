{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01e9c164",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:20:15.382713Z",
     "iopub.status.busy": "2025-03-31T01:20:15.379675Z",
     "iopub.status.idle": "2025-03-31T01:20:21.239451Z",
     "shell.execute_reply": "2025-03-31T01:20:21.229802Z",
     "shell.execute_reply.started": "2025-03-31T01:20:15.382663Z"
    }
   },
   "outputs": [],
   "source": [
    "#Model to generate Lorem Ipsum\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "612399b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:20:21.242399Z",
     "iopub.status.busy": "2025-03-31T01:20:21.241885Z",
     "iopub.status.idle": "2025-03-31T01:20:21.338897Z",
     "shell.execute_reply": "2025-03-31T01:20:21.324787Z",
     "shell.execute_reply.started": "2025-03-31T01:20:21.242362Z"
    }
   },
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "BatchSize = 128\n",
    "BlockSize = 512\n",
    "Dmodel = 512\n",
    "nheads = 12\n",
    "Dk = int(Dmodel/nheads)\n",
    "Dv = Dk\n",
    "LearningRate = 2e-4\n",
    "MaxIters = 10000\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "ContextLength=1000\n",
    "TransformerBlocks = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d39483cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:20:21.353616Z",
     "iopub.status.busy": "2025-03-31T01:20:21.352762Z",
     "iopub.status.idle": "2025-03-31T01:20:21.425683Z",
     "shell.execute_reply": "2025-03-31T01:20:21.387856Z",
     "shell.execute_reply.started": "2025-03-31T01:20:21.353572Z"
    }
   },
   "outputs": [],
   "source": [
    "#Decode string from tokens\n",
    "def decode(tokens):\n",
    "    str = []\n",
    "    index = 0\n",
    "    offset = 0\n",
    "    while index - offset < len(tokens):\n",
    "        offset = 0\n",
    "        token = tokens[index]\n",
    "        #print(token)\n",
    "        if token in Initialvocab:\n",
    "            str.append(token)\n",
    "        else:\n",
    "            #print(merges)\n",
    "            if token in merges.values():\n",
    "                 for key, value in merges.items():\n",
    "                    if value == token:\n",
    "                        #print(token)\n",
    "                        tokens.insert(index + 1, key[0])\n",
    "                        tokens.insert(index + 2, key[1])\n",
    "                        offset = 1\n",
    "                        break\n",
    "        index +=1\n",
    "    return bytes(str).decode(\"utf-8\", errors=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58c427f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:20:21.430274Z",
     "iopub.status.busy": "2025-03-31T01:20:21.427585Z",
     "iopub.status.idle": "2025-03-31T01:20:21.496628Z",
     "shell.execute_reply": "2025-03-31T01:20:21.495015Z",
     "shell.execute_reply.started": "2025-03-31T01:20:21.430214Z"
    }
   },
   "outputs": [],
   "source": [
    "def getPairFreqs(text):\n",
    "    freqs = {}\n",
    "    for pair in zip(text, text[1:]):\n",
    "        try:\n",
    "            freqs[pair] +=1\n",
    "        except KeyError:\n",
    "            freqs[pair] = 1\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f3c5cbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:20:21.546773Z",
     "iopub.status.busy": "2025-03-31T01:20:21.511691Z",
     "iopub.status.idle": "2025-03-31T01:20:21.577462Z",
     "shell.execute_reply": "2025-03-31T01:20:21.576113Z",
     "shell.execute_reply.started": "2025-03-31T01:20:21.546698Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge(text, pair, newChar):\n",
    "    newText = []\n",
    "    i=0\n",
    "    while i < len(text):\n",
    "        if i < len(text) - 1 and (text[i], text[i+1]) == (pair[0], pair[1]):\n",
    "            newText.append(newChar)\n",
    "            i+=2\n",
    "        else:\n",
    "            newText.append(text[i])\n",
    "            i+=1\n",
    "    return newText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c7d4039",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:20:21.578858Z",
     "iopub.status.busy": "2025-03-31T01:20:21.578524Z",
     "iopub.status.idle": "2025-03-31T01:20:21.846303Z",
     "shell.execute_reply": "2025-03-31T01:20:21.841596Z",
     "shell.execute_reply.started": "2025-03-31T01:20:21.578827Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load variables from pickle files (Lorem Ipsum)\n",
    "with open('XFMinitialvocab.pkl', 'rb') as f:\n",
    "    Initialvocab = pickle.load(f)\n",
    "    \n",
    "with open('XFMvocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "    \n",
    "with open('XFMvocabsize.pkl', 'rb') as f:\n",
    "    VocabSize = pickle.load(f)\n",
    "    \n",
    "with open('XFMmerges.pkl', 'rb') as f:\n",
    "    merges = pickle.load(f)\n",
    "    \n",
    "with open('XFMtrainingdata.pkl', 'rb') as f:\n",
    "    TrainingData = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97574bb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:20:21.846303Z",
     "iopub.status.busy": "2025-03-31T01:20:21.846303Z",
     "iopub.status.idle": "2025-03-31T01:20:21.865098Z",
     "shell.execute_reply": "2025-03-31T01:20:21.863870Z",
     "shell.execute_reply.started": "2025-03-31T01:20:21.846303Z"
    }
   },
   "outputs": [],
   "source": [
    "#Encode string to tokens\n",
    "def encode(text):\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "    while True:\n",
    "        freqs = getPairFreqs(tokens)\n",
    "        pairIndex = float(\"inf\")\n",
    "        pairToMerge=\"\"\n",
    "        for pair in freqs.keys():\n",
    "            if merges.get(pair, float(\"inf\")) < pairIndex:\n",
    "                pairIndex = merges.get(pair, float(\"inf\"))\n",
    "                pairToMerge = pair\n",
    "        if pairIndex == float(\"inf\"):\n",
    "            break\n",
    "        tokens = merge(tokens, pairToMerge, pairIndex)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "296a5c38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:20:21.866975Z",
     "iopub.status.busy": "2025-03-31T01:20:21.866286Z",
     "iopub.status.idle": "2025-03-31T01:20:21.950936Z",
     "shell.execute_reply": "2025-03-31T01:20:21.949605Z",
     "shell.execute_reply.started": "2025-03-31T01:20:21.866813Z"
    }
   },
   "outputs": [],
   "source": [
    "#Currently we don't use any ReGex but I will see if I want to change this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b182cf6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:20:21.952250Z",
     "iopub.status.busy": "2025-03-31T01:20:21.951915Z",
     "iopub.status.idle": "2025-03-31T01:20:22.074091Z",
     "shell.execute_reply": "2025-03-31T01:20:22.072930Z",
     "shell.execute_reply.started": "2025-03-31T01:20:21.952213Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create Training/Testing Data Batches\n",
    "def GetBatch(split):\n",
    "    offsets = []\n",
    "    for i in range(0, BatchSize):\n",
    "        offsets.append(random.randint(0, len(TrainingData) - BlockSize - 1))\n",
    "    for i in range(0, BatchSize):\n",
    "        x = torch.stack([torch.tensor(TrainingData[i:i+BlockSize]) for i in offsets])\n",
    "        y = torch.stack([torch.tensor(TrainingData[i+1:i+BlockSize+1]) for i in offsets])\n",
    "    return x, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d66389a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:20:22.079119Z",
     "iopub.status.busy": "2025-03-31T01:20:22.078511Z",
     "iopub.status.idle": "2025-03-31T01:20:22.141010Z",
     "shell.execute_reply": "2025-03-31T01:20:22.134266Z",
     "shell.execute_reply.started": "2025-03-31T01:20:22.079074Z"
    }
   },
   "outputs": [],
   "source": [
    "#Single Attention Head\n",
    "class AttentionHead(nn.Module):\n",
    "    \n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        #Learned key matrix\n",
    "        self.Wk = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((Dmodel, Dk), device=device)), requires_grad=True)\n",
    "        #Learned query matrix\n",
    "        self.Wq = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((Dmodel, Dk), device=device)), requires_grad=True)\n",
    "        #Learned value matrix\n",
    "        self.Wv = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((Dmodel, Dv), device=device)), requires_grad=True)\n",
    "    \n",
    "    def forward(self, E):\n",
    "        #print(E.shape)\n",
    "        #Dynamically checks size of E as it is not always \n",
    "        Batches, Blocks = E.shape[0], E.shape[1]\n",
    "        #Compute key, query, value matrices\n",
    "        Q = E @ self.Wq\n",
    "        K = E @ self.Wk\n",
    "        V = E @ self.Wv\n",
    "        #Masking matrix\n",
    "        M = torch.tril(torch.zeros(Blocks, Blocks)).masked_fill(torch.tril(torch.ones(Blocks, Blocks)) == 0, float(\"-inf\")).to(device)\n",
    "        #Compute attention pattern\n",
    "        AttentionPattern = torch.nn.functional.softmax(((Q @ K.transpose(-2,-1))/((Dk)**(1/2)) + M), dim = -1) @ V\n",
    "        #Residual connection\n",
    "        return AttentionPattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9414b931",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:20:22.142790Z",
     "iopub.status.busy": "2025-03-31T01:20:22.142392Z",
     "iopub.status.idle": "2025-03-31T01:20:22.233880Z",
     "shell.execute_reply": "2025-03-31T01:20:22.229497Z",
     "shell.execute_reply.started": "2025-03-31T01:20:22.142754Z"
    }
   },
   "outputs": [],
   "source": [
    "#Multi-headed Attention\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([AttentionHead() for i in range(nheads)])\n",
    "        \n",
    "    def forward(self, E):\n",
    "        #Concatenate multiple heads of attention\n",
    "        return torch.cat([head(E) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ee5be2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:20:22.233880Z",
     "iopub.status.busy": "2025-03-31T01:20:22.233880Z",
     "iopub.status.idle": "2025-03-31T01:20:22.282209Z",
     "shell.execute_reply": "2025-03-31T01:20:22.280589Z",
     "shell.execute_reply.started": "2025-03-31T01:20:22.233880Z"
    }
   },
   "outputs": [],
   "source": [
    "#Feedforward layer\n",
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ffl = nn.Sequential(\n",
    "            #Each MLP has 4* more neurons than there are dimensions .\n",
    "            nn.Linear(Dmodel, Dmodel * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(Dmodel * 4, Dmodel),\n",
    "        )\n",
    "\n",
    "    def forward(self, E):\n",
    "        return self.ffl(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55a42855",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:20:22.284874Z",
     "iopub.status.busy": "2025-03-31T01:20:22.284393Z",
     "iopub.status.idle": "2025-03-31T01:20:22.313872Z",
     "shell.execute_reply": "2025-03-31T01:20:22.308803Z",
     "shell.execute_reply.started": "2025-03-31T01:20:22.284874Z"
    }
   },
   "outputs": [],
   "source": [
    "#Transformer Block\n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.AttentionHeads = MultiHeadedAttention()\n",
    "        self.ffl = FeedForwardLayer()\n",
    "        self.LayerNorm1 = nn.LayerNorm(Dmodel, device=device)\n",
    "        self.LayerNorm2 = nn.LayerNorm(Dmodel, device=device)\n",
    "    \n",
    "    #Slightly different to my formalisation as do layernorm before blocks, not after\n",
    "    #The original paper did indeed do layernorm second, so switch it back to this\n",
    "    def forward(self, E):\n",
    "        E = E + self.AttentionHeads(self.LayerNorm1(E))\n",
    "        E = E + self.ffl(self.LayerNorm2(E))\n",
    "        return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d177a8ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:20:22.321917Z",
     "iopub.status.busy": "2025-03-31T01:20:22.321851Z",
     "iopub.status.idle": "2025-03-31T01:20:22.370748Z",
     "shell.execute_reply": "2025-03-31T01:20:22.355975Z",
     "shell.execute_reply.started": "2025-03-31T01:20:22.321875Z"
    }
   },
   "outputs": [],
   "source": [
    "#Unembedding Layer. We don't focus on the last embedding at this stage unlike in my formalisation. We can do that when we need to\n",
    "class UnembedLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        #Learned unembedding matrix\n",
    "        self.Wu = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((Dmodel, VocabSize), device=device)), requires_grad=True)\n",
    "        \n",
    "    def forward(self, E):\n",
    "        logits = E @ self.Wu\n",
    "        #Dynamically check shape of logits, since this isnt always BatchSize, BlockSize (e.g. when generating text)\n",
    "        Batches, Blocks = logits.shape[0], logits.shape[1]\n",
    "        #Converts logits to a single list of logits for compatibility with cross entropy functional\n",
    "        logits = logits.view(Batches*Blocks, VocabSize)\n",
    "        #probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "369f773b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:20:22.372597Z",
     "iopub.status.busy": "2025-03-31T01:20:22.372188Z",
     "iopub.status.idle": "2025-03-31T01:20:22.938408Z",
     "shell.execute_reply": "2025-03-31T01:20:22.936059Z",
     "shell.execute_reply.started": "2025-03-31T01:20:22.372588Z"
    }
   },
   "outputs": [],
   "source": [
    "#Transformer Implementation\n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        #Learned Embedding matrix. Requires_grad ensures We updated during backpropagation.\n",
    "        self.We = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((VocabSize, Dmodel), device=device)), requires_grad=True)\n",
    "        #Learned positional encoding matrix. Dimension n x d_model, where n = BlockSize\n",
    "        self.Wp = nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty((ContextLength, Dmodel), device=device)), requires_grad=True)\n",
    "        #Unembeding layer\n",
    "        self.UnembedLayer = UnembedLayer()\n",
    "        #Transformer blocks\n",
    "        self.Blocks = nn.Sequential(*[Block() for i in range(TransformerBlocks)])\n",
    "        \n",
    "        #Having O = None means O is optional. We don't always want targets since when generating text we don't have targets.\n",
    "    def forward(self, I, O = None):\n",
    "        #I has shape BatchSize x BlockSize\n",
    "        #One hot vector for tokens. Shape BatchSize x BlockSize x VocabSize\n",
    "        U = torch.nn.functional.one_hot(torch.tensor(I).to(device), VocabSize).float()\n",
    "        #Initial token embeddings. Shape BatchSize x BlockSize x d_model\n",
    "        E = U @ self.We\n",
    "        #Adds another dimension to Wp so that it is now 1 x BlockSize x d_model and can be added to E\n",
    "        P = self.Wp.unsqueeze(0)  \n",
    "\n",
    "        #Adds positional encoding to embedding\n",
    "        E = E + P[:, :E.shape[1], :] #truncates the positional encoding matrix to only be as long as the number of embeddings in E\n",
    "        E = self.Blocks(E)\n",
    "        #print(E[1][1])\n",
    "        logits = self.UnembedLayer(E).to(device)\n",
    "        if O is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            #Converts O to a single list of expected outputs for compatibility with cross entropy functional\n",
    "            O = O.view(BlockSize * BatchSize).to(device)\n",
    "            #Cross entropy loss calculated on the raw logits rather than Softmaxed logits.\n",
    "            #In theory, Cross entropy should be calculated on the Softmaxed logits, but\n",
    "            #the cross_entropy function in python is defined to take in raw logits\n",
    "            #If you try and pass the softmaxed logits in (as I originally tried), you will get\n",
    "            #Vanishing gradient and your network wont train\n",
    "            loss = torch.nn.functional.cross_entropy(logits, O)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generateText(self, I, Length):\n",
    "        for i in range(Length):\n",
    "            #Get predictions\n",
    "            logits, loss = self(I)\n",
    "            #Get probs\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            #Focus on prediction for next token\n",
    "            probs = probs[-1, :]\n",
    "            #Sample from next token distribution\n",
    "            nextToken = torch.multinomial(probs, num_samples = 1).unsqueeze(0)\n",
    "            #print(I)\n",
    "            #Concatenate next token to current text\n",
    "            I = torch.cat((I.to(device), nextToken.to(device)), dim=1)\n",
    "        return I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43dbecda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:20:22.940872Z",
     "iopub.status.busy": "2025-03-31T01:20:22.940315Z",
     "iopub.status.idle": "2025-03-31T01:20:25.862435Z",
     "shell.execute_reply": "2025-03-31T01:20:25.858967Z",
     "shell.execute_reply.started": "2025-03-31T01:20:22.940830Z"
    }
   },
   "outputs": [],
   "source": [
    "T = Transformer()\n",
    "T = T.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02731aa8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:20:25.865443Z",
     "iopub.status.busy": "2025-03-31T01:20:25.864402Z",
     "iopub.status.idle": "2025-03-31T01:20:27.793921Z",
     "shell.execute_reply": "2025-03-31T01:20:27.772820Z",
     "shell.execute_reply.started": "2025-03-31T01:20:25.864402Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters: 86447616\n"
     ]
    }
   ],
   "source": [
    "#Model training\n",
    "#Prints parameters\n",
    "#print(list(T.parameters()))\n",
    "#Prints number of trainable parameters\n",
    "print(\"Number of trainable parameters: \" + str((sum(p.numel() for p in T.parameters() if p.requires_grad))))\n",
    "#Create optimiser object\n",
    "optimiser = torch.optim.AdamW(T.parameters(), lr=LearningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6945b085-030a-4aa1-a881-5d982075274a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T01:20:27.816981Z",
     "iopub.status.busy": "2025-03-31T01:20:27.814433Z",
     "iopub.status.idle": "2025-03-31T01:20:50.427045Z",
     "shell.execute_reply": "2025-03-31T01:20:50.370974Z",
     "shell.execute_reply.started": "2025-03-31T01:20:27.816981Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1128/3863220517.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  U = torch.nn.functional.one_hot(torch.tensor(I).to(device), VocabSize).float()\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacty of 23.86 GiB of which 304.12 MiB is free. Process 3388977 has 830.00 MiB memory in use. Process 3389026 has 6.53 GiB memory in use. Process 3392450 has 16.22 GiB memory in use. Of the allocated memory 15.81 GiB is allocated by PyTorch, and 242.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Generate Text: Pre-Training\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#Check loss\u001b[39;00m\n\u001b[1;32m      4\u001b[0m I, O \u001b[38;5;241m=\u001b[39m GetBatch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mI\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mO\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#Generates text of given length\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 27\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, I, O)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#Adds positional encoding to embedding\u001b[39;00m\n\u001b[1;32m     26\u001b[0m E \u001b[38;5;241m=\u001b[39m E \u001b[38;5;241m+\u001b[39m P[:, :E\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], :] \u001b[38;5;66;03m#truncates the positional encoding matrix to only be as long as the number of embeddings in E\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m E \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBlocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#print(E[1][1])\u001b[39;00m\n\u001b[1;32m     29\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mUnembedLayer(E)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 14\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, E)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, E):\n\u001b[1;32m     13\u001b[0m     E \u001b[38;5;241m=\u001b[39m E \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mAttentionHeads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm1(E))\n\u001b[0;32m---> 14\u001b[0m     E \u001b[38;5;241m=\u001b[39m E \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLayerNorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m E\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m, in \u001b[0;36mFeedForwardLayer.forward\u001b[0;34m(self, E)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, E):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mE\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/activation.py:101\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:1471\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1469\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1471\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacty of 23.86 GiB of which 304.12 MiB is free. Process 3388977 has 830.00 MiB memory in use. Process 3389026 has 6.53 GiB memory in use. Process 3392450 has 16.22 GiB memory in use. Of the allocated memory 15.81 GiB is allocated by PyTorch, and 242.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "#Generate Text: Pre-Training\n",
    "\n",
    "#Check loss\n",
    "I, O = GetBatch(\"training\")\n",
    "logits, loss = T(I, O)\n",
    "print(loss)\n",
    "#Generates text of given length\n",
    "tokens = sum(T.generateText(torch.zeros(1,1).long(), 150).tolist(), [])\n",
    "print(decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13b052c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-31T01:20:50.429168Z",
     "iopub.status.idle": "2025-03-31T01:20:50.429919Z",
     "shell.execute_reply": "2025-03-31T01:20:50.429626Z",
     "shell.execute_reply.started": "2025-03-31T01:20:50.429582Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Train the model\n",
    "start_time = time.time()\n",
    "for iters in range(MaxIters + 1):\n",
    "    #GetBatches\n",
    "    I, O = GetBatch(\"training\") #This works\n",
    "    #Get loss from Transformer pass\n",
    "    logits, loss = T(I, O) \n",
    "    optimiser.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    if iters % 100 == 0:\n",
    "        new_time = time.time()\n",
    "        print(str(iters) + \": \" + str(loss))\n",
    "        print(str(new_time - start_time) + \" seconds elapsed\")\n",
    "        start_time = start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538a1d53",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-31T01:20:50.432278Z",
     "iopub.status.idle": "2025-03-31T01:20:50.432882Z",
     "shell.execute_reply": "2025-03-31T01:20:50.432668Z",
     "shell.execute_reply.started": "2025-03-31T01:20:50.432643Z"
    }
   },
   "outputs": [],
   "source": [
    "#Generate Text: Post-Training\n",
    "\n",
    "#Check loss\n",
    "I, O = GetBatch(\"training\")\n",
    "logits, loss = T(I, O)\n",
    "print(loss)\n",
    "#Generates text of given length\n",
    "tokens = sum(T.generateText(torch.zeros(1,1).long(), 150).tolist(), [])\n",
    "print(decode(tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
